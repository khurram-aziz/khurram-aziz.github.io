<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>/</id>
	<title>Khurram Aziz</title>
	<link rel="self" href="/" />
	<rights>Copyright © 2024</rights>
	<updated>2024-04-13T21:01:47Z</updated>
	<subtitle>All In All Its Just Another Brick In The Wall!</subtitle>
	<entry>
		<id>/posts/beats</id>
		<title>Beats</title>
		<link href="/posts/beats" />
		<updated>2019-07-04T00:00:00Z</updated>
		<content>&lt;p ilm_enabled="&amp;amp;gt;" auto=""&gt;&lt;b&gt;&lt;img title="image" style="margin:10px 10px 0px 0px;float:right;display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image.png" width="619" align="right" height="421"&gt;Elasticsearch Series&lt;/b&gt;   &lt;/p&gt;&lt;ul&gt;   &lt;li&gt;&lt;a href="/elk"&gt;ELK&lt;/a&gt;&lt;/li&gt;    &lt;li&gt;&lt;a href="/kibana"&gt;Kibana: DevTools&lt;/a&gt;&lt;/li&gt;    &lt;li&gt;This Post&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Beats is Elastic� platform for data shippers. From previous posts we know that Elasticsearch and Logstash are servers and we can send data (usually time series data) to them. If data needs some input/filter/output parsing Logstash is there or else we can submit data directly to Elasticsearch. Beats are single-purpose data shippers that we can deploy on systems / machines / containers acting as clients for either Logstash or Elasticsearch � and then centralize data either directly to Elasticsearch or through Logstash. There are different beats available like &lt;a href="https://www.elastic.co/products/beats/filebeat" target="_blank"&gt;Filebeat&lt;/a&gt; that can harvest files and is mostly used to feed log files into Elasticsearch, &lt;a href="https://www.elastic.co/products/beats/winlogbeat" target="_blank"&gt;Winlogbeat&lt;/a&gt; that can read from Windows� event log channels and ship the event data into the Elasticsearch� structured indexes, &lt;a href="https://www.elastic.co/products/beats/metricbeat" target="_blank"&gt;Metricbeat&lt;/a&gt; that can collect CPU, memory and other system metrics and can log statistics into Elasticsearch and &lt;a href="https://www.elastic.co/products/beats/packetbeat" target="_blank"&gt;Packetbeat&lt;/a&gt; can monitor network services using different network protocols and can record application latency, response times, SLA performances and user access patterns etc&lt;/p&gt;  &lt;p&gt;Beats are "lightweight" and easy to deploy; they are developed using Go; so there is just a binary along with its configuration file to deploy and have no external dependencies; ideal for container deployment and use limited system resources.&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Take a look at &lt;a href="https://www.elastic.co/products/beats" target="_blank"&gt;https://www.elastic.co/products/beats&lt;/a&gt; for complete first party Beats list&lt;/li&gt; &lt;/ul&gt;  &lt;h1&gt;Modules and libbeat&lt;/h1&gt;  &lt;p&gt;Many beats come with modules / plugins to help it collect and parse/filter data; for instance Filebeat comes with Apache, IIS, Nginx, MySQL, PostgreSQL, Redis, Netflow, Cisco and many others; using these it can harvest relevant log files; for example using IIS module we can feed IIS log files into Logstash or Elasticsearch. Winlogbeat comes with Sysmon Module. Sysinternals� System Monitor utility service that log system activity events into its own event log channel that Winlogbeat can then process.&lt;/p&gt;  &lt;p&gt;Beats are open source and code is available on Github; all the Beats uses libbeat; a Go framework from Elastic to create Beats. All the official Beats uses it that makes all of them to feel and work in consistent and uniform way; for instance for configuration they all use YML files and now with v7.2 releases that can run Javascript code to process events and transform data �at the edge� (industrial buzz). The beats modules also uses this same Javascript option for instance Winlogbeat�s Sysmon module is configured like this:&lt;/p&gt;  &lt;p&gt;&lt;img title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_3.png" width="569" height="140"&gt;&lt;/p&gt;  &lt;p&gt;There are many open source beats from community using this same libbeat; for example MQTTBeat (&lt;a title="https://github.com/nathan-K-/mqttbeat" href="https://github.com/nathan-K-/mqttbeat" target="_blank"&gt;https://github.com/nathan-K-/mqttbeat&lt;/a&gt;) can be quite useful in IoT implementation. It can read MQTT topics and can save messages into Elasticsearch (or process them through Logstash). HttpBeat (&lt;a href="https://github.com/christiangalsterer/httpbeat" target="_blank"&gt;https://github.com/christiangalsterer/httpbeat&lt;/a&gt;) call http endpoints (POST/GET requests at URLs etc) at specified intervals and can record JSON outputs and HTTP server metrics. HttpBeat is now a Http module of Metricbeat&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Take a look at &lt;a href="https://www.elastic.co/guide/en/beats/libbeat/master/index.html" target="_blank"&gt;https://www.elastic.co/guide/en/beats/libbeat/master/index.html&lt;/a&gt; for libbeat reference, &lt;a href="https://www.elastic.co/guide/en/beats/libbeat/master/community-beats.html" target="_blank"&gt;https://www.elastic.co/guide/en/beats/libbeat/master/community-beats.html&lt;/a&gt; for the community beats and &lt;a href="https://github.com/elastic/beats" target="_blank"&gt;https://github.com/elastic/beats&lt;/a&gt; for libbeat Go framework code/library&lt;/li&gt; &lt;/ul&gt;  &lt;h1&gt;Index Lifecycle Management&lt;/h1&gt;  &lt;p&gt;By default the Beats creates indexes for each day, the beat name followed by its version and then the date; for instance Filebeat will create following indexes:&lt;/p&gt;  &lt;p&gt;&lt;a href="/images/Beats_9B7B/image_4.png"&gt;&lt;img title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_thumb.png" width="582" height="311"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;p&gt;Version 6.6.0 the Elastic stack introduced Index Lifecycle Management; ILM; using this feature in Elasticsearch we can manage indices as they age. For example, instead of creating daily indices where index size can vary based on the number of Beats and number of events sent, use an index lifecycle policy to automate a rollover to a new index when the existing index reaches a specified size or age. From v7 releases, Beats now uses ILM by default when it connects to Elasticsearch that supports life cycle management, Beats create their default policy automatically in the Elasticsearch and applies it to any indices created by them. We can then view and edit the policy in the Index lifecycle policies User interface in Kibana.&lt;/p&gt;  &lt;p&gt;&lt;img title="image" style="display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_5.png" width="628" height="484"&gt;&lt;/p&gt;  &lt;p&gt;We can change this behaviour by changing elasticsearch  to true or false in Beat�s configuration�s output section&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Take a look at &lt;a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html" target="_blank"&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/index-lifecycle-management.html&lt;/a&gt; for more information&lt;/li&gt; &lt;/ul&gt;  &lt;h1&gt;Elastic Common Schema&lt;/h1&gt;  &lt;p&gt;Beats gather the logs and metrics from your unique environments and document them with essential metadata from hosts. Here is an example of Filebeat entry.&lt;/p&gt;  &lt;p&gt;&lt;img title="image" style="display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_6.png" width="620" height="484"&gt;&lt;/p&gt;  &lt;p&gt;Elastic Common Schema (ECS) is&amp;nbsp; a specification that provides a consistent and customizable way to structure the data in Elasticsearch, facilitating the analysis of data from diverse sources. With ECS, analytics content such as dashboards and machine learning jobs can be applied more broadly, searches can be crafted more narrowly, and field names are easier to remember. Imagine searching for a specific user within data originating from multiple sources. Just to search for this one field, you would likely need to account for multiple field names, such as user, username, nginx.access.user_name, and login. Implementing ECS unifies all modes of analysis available in the Elastic Stack, including search, drill-down and pivoting, data visualization, machine learning-based anomaly detection, and alerting. When fully adopted, users can search with the power of both unstructured and structured query parameters. It is an open source specification available at &lt;a title="https://github.com/elastic/ecs" href="https://github.com/elastic/ecs" target="_blank"&gt;https://github.com/elastic/ecs&lt;/a&gt; that defines a common set of document fields for data ingested into Elasticsearch.&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Take a look at &lt;a title="https://www.elastic.co/guide/en/ecs/current/index.html" href="https://www.elastic.co/guide/en/ecs/current/index.html" target="_blank"&gt;https://www.elastic.co/guide/en/ecs/current/index.html&lt;/a&gt; and read &lt;a href="https://www.elastic.co/blog/introducing-the-elastic-common-schema" target="_blank"&gt;https://www.elastic.co/blog/introducing-the-elastic-common-schema&lt;/a&gt; for more information.&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Starting with version 7.0, all Beats and Beats modules generate ECS format events by default. This means adopting ECS is as easy as upgrading to Beats 7.0. All Beats module dashboards are already modified to make use of ECS.&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;For more information about v7 Beats release read &lt;a href="https://www.elastic.co/blog/beats-7-0-0-released" target="_blank"&gt;https://www.elastic.co/blog/beats-7-0-0-released&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;h1&gt;Dashboards&lt;/h1&gt;  &lt;p&gt;If we run the beat with �help; we will often see �setup� option that creates the index template (that we see in the previous post are used to define fields and configure how the new indexes will be created), Machine Learning Job (if you have X-Pack the commercial addon from Elastic) and Dashboards. Beats can access Kibana if you have specified its location in their configuration file and create Kibana Index Patterns, Visualization and Dashboards&lt;/p&gt;  &lt;p&gt;&lt;img title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_7.png" width="647" height="441"&gt;&lt;/p&gt;  &lt;p&gt;For example running winlogbeat setup will create the Index Patterns, Visualizations and Dashboard (if we have Kibana URL in its configuration YML file)&lt;/p&gt;  &lt;p&gt;&lt;a href="/images/Beats_9B7B/image_8.png"&gt;&lt;img title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_thumb_3.png" width="644" height="341"&gt;&lt;/a&gt;&lt;a href="/images/Beats_9B7B/image_9.png"&gt;&lt;img title="image" style="display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_thumb_4.png" width="506" height="96"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;p&gt;We can view the Visualizations and Dashboards in Kibana (and learn how they have made it and use this learning to create Visualizations and Dashboards for our other data that we are pushing into Elasticsearch)&lt;/p&gt;  &lt;p&gt;&lt;a href="/images/Beats_9B7B/image_10.png"&gt;&lt;img title="image" style="display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_thumb_5.png" width="501" height="484"&gt;&lt;/a&gt;&lt;img title="image" style="display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_11.png" width="644" height="271"&gt;&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Note that I am using v7x Winlogbeat and its using ECS and has created Visualization and Dashboards with ECS in name to avoid clash from v6x and earlier Winlogbeat that you might have&lt;/li&gt; &lt;/ul&gt;  &lt;h1&gt;Debugging and Service Installation&lt;/h1&gt;  &lt;p&gt;Using beat test we can test the configuration; for example winlogbeat test config we can test the configuration file and using winlogbeat test output we can ensure Elasticsearch is accessible.&lt;/p&gt;  &lt;p&gt;&lt;img title="image" style="display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_12.png" width="521" height="211"&gt;&lt;/p&gt;  &lt;p&gt;We can run the Beats interactively using its binary; it will report any error on the console and we can fix it accordingly. Once everything is in order; we can install the Beat (Winlogbeat, Filebeat or other); the Beats usually comes with instruction or scripts to do this; for instance Winlogbeat comes with Powershell script to install / uninstall it as Windows Service.&lt;/p&gt;  &lt;p&gt;&lt;a href="/images/Beats_9B7B/image_13.png"&gt;&lt;img title="image" style="display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_thumb_6.png" width="951" height="340"&gt;&lt;/a&gt;&lt;/p&gt; &lt;a href="/images/Beats_9B7B/image_14.png"&gt;&lt;img title="image" style="margin:10px 10px 0px 0px;float:right;display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_thumb_7.png" width="644" align="right" height="240"&gt;    &lt;br&gt;&lt;/a&gt;  &lt;h1&gt;Kibana and X-Pack&lt;/h1&gt;  &lt;p&gt;Recent versions of Kibana now has dedicated appplications for some Beats, for instance there is Uptime application that works with Hearbeat, Logs that work with Filebeat and SIEM (Security Information &amp;amp; Event Management)&lt;/p&gt;  &lt;p&gt;Elastic SIEM is being introduced as a beta in the 7.2 release of the Elastic Stack. SIEM Kibana application is an interactive workspace for security teams to triage events and perform initial investigations. It enables analysis of host-related and network-related security events as part of alert investigations. With its Timeline Event Viewer the analysts can gather and store evidence of an attack, pin and annotate relevant events, and comment on and share their findings. It uses the data that follows the ECS format being pushed by Auditbeat, Filebeat, Winlogbeat and Packetbeat (in its first release)&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Read &lt;a href="https://www.elastic.co/blog/introducing-elastic-siem" target="_blank"&gt;https://www.elastic.co/blog/introducing-elastic-siem&lt;/a&gt; for more information on Elastic SIEM&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;a href="/images/Beats_9B7B/image_15.png"&gt;&lt;img title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image_thumb_8.png" width="644" height="321"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;p&gt;Elastic used to offer Sheild, Watcher and Marvel commercial plugins that adds features like Security and Alerting to ELK setup. They then merged these products into X-Pack which is an Elastic Stack extension that bundles security, alerting, monitoring, reporting, and graph capabilities into one easy-to-install package. While the X-Pack components are designed to work together seamlessly, you can easily enable or disable the features you want to use. Note that this is a commercial product and they in &lt;a href="https://www.elastic.co/blog/doubling-down-on-open" target="_blank"&gt;early 2018 they announced&lt;/a&gt; opening the code of X-Pack features - security, monitoring, alerting, graph, reporting, dedicated APM UIs, Canvas, Elasticsearch SQL, Search Profiler, Grok Debugger, Elastic Maps Service zoom levels, and machine learning. Many of these new additions in Kibana are result of this and at the same time they are also adding new Kibana applications like SIEM. So its best to keep updating your ELK setup to avail all these new features.     &lt;/p&gt;&lt;p&gt;&lt;/p&gt;
</content>
		<summary>&lt;p ilm_enabled="&amp;amp;amp;gt;" auto=""&gt;&lt;b&gt;&lt;img title="image" style="margin:10px 10px 0px 0px;float:right;display:inline;background-image:none;" border="0" alt="image" src="/images/Beats_9B7B/image.png" width="619" align="right" height="421"&gt;Elasticsearch Series&lt;/b&gt;   &lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/kibana</id>
		<title>Kibana DevTools</title>
		<link href="/posts/kibana" />
		<updated>2019-06-01T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;b&gt;Elasticsearch Series&lt;/b&gt;&amp;nbsp;   &lt;/p&gt;&lt;ul&gt;             &lt;li&gt;&lt;a href="/elk"&gt;ELK&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;This Post&lt;/li&gt;  &lt;li&gt;&lt;a href="/beats"&gt;Beats&lt;/a&gt;&lt;/li&gt;  &lt;/ul&gt;      &lt;p&gt;Kibana allows us to work with Elasticsearch in most productive way. From the previous post; lets expose our Elasticsearch server and using its REST APIs; create an “index” and a document in there directly using CURL&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/1-curl.png"&gt;&lt;img title="1-curl" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="1-curl" width="644" height="311" src="/images/Elasticsearch-and-Kibana_CA2C/1-curl_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;You will realize; its quite tedious to work with Elasticsearch this way; especially in Windows Command Prompt where you have to escape the quote character; this is where Dev Tools in Kibana can help us work better.&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/2-devtools.png"&gt;&lt;img title="2-devtools" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="2-devtools" width="590" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/2-devtools_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;The Dev Tools has an option to copy the step as CURL command. Lets continue our API exploration by creating couple of more orders and then searching them&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/3a.png"&gt;&lt;img title="3a" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="3a" width="525" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/3a_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/3b.png"&gt;&lt;img title="3b" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="3b" width="525" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/3b_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/3c.png"&gt;&lt;img title="3c" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="3c" width="525" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/3c_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/3d.png"&gt;&lt;img title="3d" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="3d" width="525" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/3d_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Notice there are special endpoints under different VERBs; like _search in GET to query the data; also notice that Kibana auto completes and we can discover these special urls/endpoints&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;Elasticsearch provides a full Query Domain Specific Language based on JSON to define queries. The query consists of two types of clauses, Leaf query clauses and Compound query clauses. You can learn about this DSL @ &lt;a target="_blank" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html"&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl.html&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;We also have the Management section; from where we can manage not only Kibana but also Elasticsearch, in recent versions they have improved this section a lot and have brought over many components from X-PACK (the Elastic.co’s commercial addon) to the free edition. You will notice that our indexes are not healthy; its because the default settings of the indexes dictates to have multiple shards (distributing data across many buckets) and to have replicas. Given we have a single node; we can change the index settings by putting our new index settings document under /_template having appropriate index_patterns&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/4a.png"&gt;&lt;img title="4a" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="4a" width="538" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/4a_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/4b.png"&gt;&lt;img title="4b" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="4b" width="538" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/4b_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;With the above number_of_shards and number_of_replicas settings; if we delete the existing indexes (using Index Management) and recreate them; we should now see healthy / green indexes&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;We can find the above created template and other templates using GET /_cat/templates endpoint. To view the selected template; use GET /_template/template-name endpoint. In previous post we used Logstash; it creates a template for its indexes as well; lets investigate this template.&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/5a.png"&gt;&lt;img title="5a" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="5a" width="601" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/5a_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/5b.png"&gt;&lt;img title="5b" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="5b" width="601" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/5b_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Notice the mappings section; and how geoip field is defined&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;In the previous post in which we used geoip plugin to transform the ufw_src_ip field to user friendly geolocation fields and drew them on the Map. It could only happen if we use logstash-* index names; if we want different index names; say ufw-&lt;em&gt;; we can now add a template for ufw-&lt;/em&gt; pattern having this geoip field definition; and we should be able to use the Map Visualization with our custom ufw-* indexes&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;As an exercise; go ahead and change index name from logstash-* to ufw-* in the work done in previous post&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;Our /orders index has an issue; the orderAmount field is not numeric data type and this becomes issue when using range queries; for instance if we search for orders having more than or equal to 500 amount; an order having amount 99 will also appear; because as string 99 =&amp;gt; 500. To fix this issue; we can either use template having appropriate mappings section or we can use mappings when creating an index. If we have multiple indexes on some pattern; say orders-&lt;em&gt;; we can search across them using GET /pattern/_search endpoint&lt;/em&gt;&lt;/p&gt;&lt;em&gt;      &lt;p&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/6a.png"&gt;&lt;img title="6a" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="6a" width="454" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/6a_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/6b.png"&gt;&lt;img title="6b" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="6b" width="601" height="484" src="/images/Elasticsearch-and-Kibana_CA2C/6b_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;Given we now have the properly typed fields, we can now use Aggregations&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/7.png"&gt;&lt;img title="7" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="7" width="644" height="421" src="/images/Elasticsearch-and-Kibana_CA2C/7_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Elasticsearch has comprehensive Aggregation framework that we can use to build simple to complex data summaries; visit &lt;a title="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html" target="_blank" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html"&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html&lt;/a&gt; for more details&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;There are some interesting endpoints under GET /_cat; for instance _cat/health for the cluster health; _cat/nodes for the nodes health in the cluster and _cat/indices for listing indexes along with their health. If JSON formatted data is not that readable; we can postfix these endpoints with ?v and it will give us the information in the plain text&lt;/p&gt;      &lt;p&gt;&lt;img title="8" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="8" width="1392" height="358" src="/images/Elasticsearch-and-Kibana_CA2C/8.png"&gt;&lt;/p&gt;      &lt;/em&gt;&lt;p&gt;&lt;em&gt;To use Kibana’s Discover and Visualize tools; we can define the Kibana Index pattern say orders-&lt;/em&gt; so it can include our orders-YYYY.MM.DD Elasticsearch indexes; and using Discover tool; we can view and use Kibana’s Query Language (KQL) which is introduced in recent versions of Kibana or Apache Lucene syntax to search / query our data&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/9a.png"&gt;&lt;img title="9a" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="9a" width="644" height="383" src="/images/Elasticsearch-and-Kibana_CA2C/9a_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Elasticsearch-and-Kibana_CA2C/9b.png"&gt;&lt;img title="9b" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="9b" width="644" height="383" src="/images/Elasticsearch-and-Kibana_CA2C/9b_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;The queries used above are added into Elk folder @ &lt;a target="_blank" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Go ahead and try to use Visualization and Dashboard tools&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;&lt;img title="10" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" border="0" alt="10" width="928" height="783" src="/images/Elasticsearch-and-Kibana_CA2C/10.png"&gt;&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;b&gt;Elasticsearch Series&lt;/b&gt;    &lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/elk</id>
		<title>ELK</title>
		<link href="/posts/elk" />
		<updated>2019-05-18T00:00:00Z</updated>
		<content>&lt;table cellspacing="10" cellpadding="10"&gt;        &lt;tbody&gt;&lt;tr&gt;          &lt;td&gt;&lt;b&gt;Time Series Databases&lt;/b&gt;            &lt;ul&gt;              &lt;li&gt;&lt;a href="/prometheus"&gt;Prometheus&lt;/a&gt;&lt;/li&gt;              &lt;li&gt;&lt;a href="/influxdb"&gt;InfluxDB&lt;/a&gt;&lt;/li&gt;              &lt;li&gt;This Post&lt;/li&gt;          &lt;/ul&gt;        &lt;/td&gt;    &lt;td&gt;&lt;b&gt;Elasticsearch Series&lt;/b&gt;            &lt;ul&gt;              &lt;li&gt;This Post&lt;/li&gt;              &lt;li&gt;&lt;a href="/kibana"&gt;Kibana: DevTools&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a href="/beats"&gt;Beats&lt;/a&gt;&lt;/li&gt;          &lt;/ul&gt;        &lt;/td&gt;      &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;    &lt;p&gt;If you ever managed a Linux server or developed application for it; you probably know about Splunk; it captures, indexes and correlates real-time data in a searchable repository from which it can generate graphs, reports, alerts, dashboards and visualizations. There exists free version that one can try out or even use in production, it comes with 500Mb / day indexing limit, which is enough for many applications. Splunk also has API and allows third party applications, there is even official &lt;a target="_blank" href="https://github.com/splunk/splunk-sdk-csharp"&gt;C# SDK&lt;/a&gt; in case its your preferred development platform; seeing the Github activity of its repository seems like nothing much going on there lately. There exists an open source and much better alternative; Elasticsearch. Its open source, actively developed and maintained, strong community and ecosystem and offers not one but two .NET client libraries &lt;a target="_blank" href="https://github.com/elastic/elasticsearch-net"&gt;NEST and Elasticsearch.NET&lt;/a&gt; and they are actively maintained and developed.&lt;/p&gt;      &lt;p&gt;&lt;img width="387" height="394" align="right" style="margin:10px 10px 0px 0px;float:right;display:inline;" src="https://www.elastic.co/static/images/elk/elk-stack-elkb-diagram.svg"&gt;"ELK" is the acronym for three open source projects: Elasticsearch, Logstash, and Kibana. Elasticsearch is a search and analytics engine. Logstash is a server‑side data processing pipeline that ingests data from multiple sources simultaneously, transforms it, and then sends it to a "stash" like Elasticsearch. Kibana lets users visualize data with charts and graphs in Elasticsearch. Elasticsearch is the heart of this stack, its an open source, distributed, RESTful, JSON-based search engine based on the Apache Lucene information retrieval library. Its developed in Java and provides distributed and multitenancy out of the box. There are commercially supported versions and addons. Elastic, the company behind it provides cloud instances and other leading cloud provides support it as well. Official Elasticsearch clients are available in Java, .NET (C#), PHP, Python, Apache Groovy, Ruby and many other languages. All this makes Elasticsearch, the most popular enterprise search engine.&lt;/p&gt;      &lt;p&gt;Out of box scalability due to built sharding (the index can be divided into shards and each shard can have zero or more replicas across multiple Elasticsearch nodes), snapshot and restore features, storing data as JSON and exposing it RESTfully, indexing and search capability of Elasticsearch makes it fantastic option for structured and unstructured data including free text, system logs and more. It can be used to search &lt;/p&gt;      &lt;p&gt;Elasticsearch is highly API driven and provides strong support for storing time series data and Kibana offers Time Filters and together they can be used as Time Series database and visualization tools. Given Elasticsearch has strong support of structured and unstructured data, we can store much information in the time series along some metrics values.&lt;/p&gt;      &lt;p&gt;&lt;img width="535" height="317" title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" alt="image" border="0" src="/images/ELK_BA7A/image.png"&gt;&lt;/p&gt;      &lt;p&gt;For this post; I am going to use ELK as Splunk alternative. Splunk offers a SYSLOG server where we can push system logs. Linux logs a large amount of events to the disk, where they’re mostly stored in the /var/log directory in plain text. Most log entries go through the system logging daemon, syslogd, and are written to the system log. The logger utility allows you to quickly write a message to your system log with a single, simple command.&lt;/p&gt;      &lt;p&gt;&lt;img width="1053" height="327" title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" alt="image" border="0" src="/images/ELK_BA7A/image_3.png"&gt;&lt;/p&gt;      &lt;p&gt;SYSLOG is a standard for message logging. It allows separation of the software that generates messages, the system that stores them, and the software that reports and analyzes them. Each message is labeled with a facility code, indicating the software type generating the message, and assigned a severity level. We may use syslog for system management and security auditing as well as general informational, analysis, and debugging messages. A wide variety of devices, such as printers, routers, and message receivers across many platforms use the syslog standard. Implementations of syslog exist for many operating systems. Many Unix and Unix like OS including Ubuntu comes with a service called rsyslogd that can forward the system log messages to IP network including SYSLOG server. On Ubuntu its configuration file is located at /etc/rsyslog.d/50-default.conf&lt;/p&gt;      &lt;p&gt;&lt;img width="534" height="110" title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" alt="image" border="0" src="/images/ELK_BA7A/image_4.png"&gt;&lt;/p&gt;      &lt;p&gt;We can add an entry &lt;b&gt;*.* @Your-Syslog-Server:514&lt;/b&gt; entry there to send all log entries to some SYSLOG server. &lt;a target="_blank" href="https://github.com/jchristn/WatsonSyslogServer"&gt;Watson Syslog Server&lt;/a&gt; is descent and simple C# code that we can modify to push syslog messages to Elasticsearch using NEST or Elasticsearch.NET client libraries (or push the syslog data to some other database of your choice)&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;With *.* we are sending all log files; we can filter here which logs we are interested; for instance we can use cron.* to send just cron logs&lt;/li&gt;        &lt;li&gt;514 is the UDP port of your syslog; if your syslog server is listening on some other port; change it accordingly&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;&lt;img width="639" height="345" title="image" align="right" style="float:right;display:inline;background-image:none;" alt="image" border="0" src="/images/ELK_BA7A/image_5.png"&gt;iptables is a user-space utility program that allows a system administrator to configure the tables provided by the Linux kernel firewall (implemented as different Netfilter modules) and the chains and rules it stores. Different kernel modules and programs are currently used for different protocols; iptables applies to IPv4, ip6tables to IPv6, arptables to ARP, and ebtables to Ethernet frames. UFW, or Uncomplicated Firewall, is an interface to iptables that is geared towards simplifying the process of configuring a firewall. While iptables is a solid and flexible tool, it can be difficult for beginners to learn how to use it to properly configure a firewall.&lt;/p&gt;      &lt;p&gt;On Ubuntu, we can install ufw using apt, dont forget to allow SSH before enabling it and check you can sush your system after enabling it to avoid getting yourself locked out. The nice thing UFW does is that it also add log rules, so when it blocks something they also gets logged into the system log that we are forwarding to a SYSLOG server. So we now have a setup that blocked traffic will get reported at our syslog server&lt;/p&gt;      &lt;p&gt;&lt;img width="644" height="97" title="image" style="margin:10px 10px 0px 0px;display:inline;background-image:none;" alt="image" border="0" src="/images/ELK_BA7A/image_6.png"&gt;&lt;/p&gt;      &lt;p&gt;Logstash is an open source, server-side data processing pipeline that can ingest data from different sources, transforms it, and then sends it to “stash”; Elasticsearch is preferred choice naturally. It supports a variety of inputs that pull in events or can listen where events are submitted, can ingest from logs, metrics, web applications and other data stores, all in continuous, streaming fashion. It can then filters parse each event, identify named fields to build structure, and transform them to converge on a common format for more powerful analysis and business value. It then route data where we want, giving us the flexibility and choice.&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;&lt;a target="_blank" href="https://www.elastic.co/guide/en/logstash/current/input-plugins.html"&gt;Logstash Input Plugins&lt;/a&gt;; they enable a specific source of events to be read by Logstash&lt;/li&gt;        &lt;li&gt;&lt;a title="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html" target="_blank" href="http://www.elastic.co/guide/en/logstash/current/filter-plugins.html"&gt;Filter Plugins&lt;/a&gt;; they perform intermediary processing on an event. Filters are often applied conditionally depending on the characteristics of the event&lt;/li&gt;        &lt;li&gt;&lt;a title="https://www.elastic.co/guide/en/logstash/current/output-plugins.html" target="_blank" href="https://www.elastic.co/guide/en/logstash/current/output-plugins.html"&gt;Output Plugins&lt;/a&gt;; they send event data to a particular destination. Outputs are the final stage in the event pipeline.&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;There can be multiple pipelines in given Logstash instance; one simple Hello World pipeline looks like this:&lt;/p&gt;      &lt;p&gt;&lt;img width="834" height="337" title="image" style="display:inline;background-image:none;" alt="image" border="0" src="/images/ELK_BA7A/image_7.png"&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;heartbeat input plugin generating test even, Hello from Logstash! message each 50sec&lt;/li&gt;        &lt;li&gt;There is no filter&lt;/li&gt;        &lt;li&gt;elasticsearch output filter writing the event to Elasticsearch&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;There is a &lt;a target="_blank" href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-syslog.html"&gt;Syslog input plugin&lt;/a&gt; using which we can make Logstash acting as Syslog server receiving logs/events and a &lt;a target="_blank" href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html"&gt;Grok filter plugin&lt;/a&gt; that can parse unstructured log data into something structured and queryable. It is perfect for syslog and other such logs type events. Logstash comes with &lt;a target="_blank" href="https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns"&gt;120 parsing patterns&lt;/a&gt; that we can use and can add our own as well.&lt;/p&gt;      &lt;p&gt;With a Logstash pipeline like below; we can listen to Linux system logs at 5000 udp as syslog server. We can then use grok plugin in the filter section and parse the syslog message.&lt;/p&gt;  &lt;script src="//gist-it.appspot.com/github/khurram-aziz/HelloDocker/blob/master/Elk/logstash-syslog.conf"&gt;&lt;/script&gt;    &lt;p&gt;In the example above; if grok pattern succeeds the rich fields will get created defined in the pattern. We can detect this using _grokparsefailure tag that grok plugin will add. If its parsing; we are applying mutate filter plugin to remove message field that gets created in the first step. We can further parse the UFW syslog message and extract the source IP and apply geoip filter that will add Geolocation fields.&lt;/p&gt;      &lt;p&gt;In output, if event source was syslog input plugin and we were able to parse the message; we are writing to syslog-* index at Elasticsearch otherwise writing to logstash-* index. UFW parsed messages are logged into logstash-* index&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;All documents in Elasticsearch are stored in some index&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;Kibana lets you visualize your Elasticsearch data and navigate the Elastic Stack. We can query/filter and view Elasticsearch data in tabular and graphical forms. We first define Kibana index; for instance logstash-* and optionally specify the time field to apply time filters on our time series data. If everything is in order we can quickly find out in Kibana from where traffic is being blocked (if any)&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/ELK_BA7A/image_9.png"&gt;&lt;img width="619" height="484" title="image" style="display:inline;background-image:none;" alt="image" border="0" src="/images/ELK_BA7A/image_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/ELK_BA7A/image_10.png"&gt;&lt;img width="633" height="484" title="image" style="display:inline;background-image:none;" alt="image" border="0" src="/images/ELK_BA7A/image_thumb_3.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;All required files are available in Elk folder @ &lt;a href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt;   &lt;/ul&gt;
</content>
		<summary>&lt;p&gt;If you ever managed a Linux server or developed application for it; you probably know about Splunk; it captures, indexes and correlates real-time data in a searchable repository from which it can generate graphs, reports, alerts, dashboards and visualizations. There exists free version that one can try out or even use in production, it comes with 500Mb / day indexing limit, which is enough for many applications. Splunk also has API and allows third party applications, there is even official &lt;a target="_blank" href="https://github.com/splunk/splunk-sdk-csharp"&gt;C# SDK&lt;/a&gt; in case its your preferred development platform; seeing the Github activity of its repository seems like nothing much going on there lately. There exists an open source and much better alternative; Elasticsearch. Its open source, actively developed and maintained, strong community and ecosystem and offers not one but two .NET client libraries &lt;a target="_blank" href="https://github.com/elastic/elasticsearch-net"&gt;NEST and Elasticsearch.NET&lt;/a&gt; and they are actively maintained and developed.&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/telegraf</id>
		<title>Telegraf</title>
		<link href="/posts/telegraf" />
		<updated>2018-06-23T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img width="611" height="480" align="right" style="margin:10px 10px 0px 0px;float:right;display:inline;" src="/images/0af903c6ed63_A51F/Tick-Stack-Complete.png"&gt;  &lt;/p&gt;&lt;p&gt;&lt;b&gt;TICK Series&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;             &lt;li&gt;&lt;a href="/influxdb"&gt;InfluxDB&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;This Post&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Telegraf is the daemon written in Go for collecting, processing, aggregating and writing metrics. It has plugins through which it offers integrations to variety of metric sources. It can pull the metrics from third-party APIs as well and can even listen for metrics via StatsD and Kafka consumer services. It then inserts the collected metrics to InfluxDB and can even push collected metrics data into Graphite, Kafka, MQTT and many others through its output plugins. It also has processor plugins through which we can transform, decorate and filter the collected metrics and we can also aggregate these collected metrics using the aggregator plugins. There are over hundred of these four types of plugins and one can write ones own; these plugins make Telegraf very extendable.&lt;/p&gt;  &lt;p&gt;Visit &lt;a title="https://github.com/influxdata/telegraf" href="https://github.com/influxdata/telegraf" target="_blank"&gt;https://github.com/influxdata/telegraf&lt;/a&gt; for the complete list of plugins. For this post; I am going to use &lt;a href="https://github.com/influxdata/telegraf/tree/master/plugins/inputs/snmp" target="_blank"&gt;SNMP&lt;/a&gt; plugin. We will be polling Temperature and Network Interface Traffic from Mikrotik Routerboards. Simple Network Management Protocol (SNMP) is an Internet Standard protocol for collecting and organizing information about managed devices on IP networks and for modifying that information to change device behavior. SNMP uses an extensible design which allows applications to define their own hierarchies. These hierarchies, are described as a management information base (MIB). MIBs describe the structure of the management data of a device subsystem; they use a hierarchical namespace containing object identifiers (OID). Each OID identifies a variable that can be read or set via SNMP. We will not be using MIBs; we can use official &lt;a href="https://hub.docker.com/_/telegraf" target="_blank"&gt;Telegraf Docker Image&lt;/a&gt; as is; otherwise one need to install SNMP-MIBs in the container. The Telegraf SNMP plugin supports both the SNMP-GET and SNMP-WALK. We can retrieve one or more values using SNMP-GET and can use these retrieved values as tags with GET (Field) or WALK (Table) metrices. We will be retrieving the device host name and use it as a tag with temperature as well as interface metrices&lt;/p&gt;  &lt;p&gt;Mikrotik Routerboard OS has two temperature OIDs; one for the sensor in its chassis and the other for its CPU temperature. The bandwidth interface information can be retreived by walking the well known OIDs for interface names, their bytes in and out counters. These SNMP configurations are made in the telegraf.conf file�s [[input.snmp]] sections. In the configuration files� [output] section we specify where we want to push the retrieved metrices. Lets create a telegraf.conf file and add Telegraf service into our Docker Compose file we created in InfluxDB post&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;All required files are available in Tick folder @ &lt;a title="https://github.com/khurram-aziz/HelloDocker" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt;    &lt;li&gt;If you are new to Docker / Docker-Compose; visit &lt;a title="docker-compose.html" href="/docker-compose" target="_blank"&gt;docker-compose.html&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;a href="/images/Telegraf_B8C5/docker-compose_3.png"&gt;&lt;img width="597" height="480" title="docker-compose" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="docker-compose" src="/images/Telegraf_B8C5/docker-compose.png" border="0"&gt;       &lt;br&gt;&lt;/a&gt;&lt;a href="/images/Telegraf_B8C5/telegraf-conf_3.png"&gt;&lt;img width="513" height="480" title="telegraf-conf" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="telegraf-conf" src="/images/Telegraf_B8C5/telegraf-conf.png" border="0"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;We are retrieving values from multiple SNMP agents each 60seconds&lt;/li&gt;    &lt;li&gt;The measurement will be named �rb� for the fields and rb-interfaces for the table&lt;/li&gt;    &lt;li&gt;�rb� measurement will have two temperature values&lt;/li&gt;    &lt;li&gt;�rb-interfaces� will have the in/out counter values; the retrieved interface name will be used as tag&lt;/li&gt;    &lt;li&gt;The hostname of the device will be used as tag in both measurements&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Running docker-compose up; we can bring our setup online and after a while; we should be able to see the measurements that Telegraf is pushing to InfluxDB�s telegraf database in the Chronograf. If you are not following �TICK series� posts and has landed on this post directly; please refer to &lt;a href="/influxdb" target="_blank"&gt;InfluxDB&lt;/a&gt; post for details on InfluxDB and Chronograf&lt;/p&gt;  &lt;p&gt;&lt;a href="/images/Telegraf_B8C5/chronograf_3.png"&gt;&lt;img width="640" height="476" title="chronograf" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="chronograf" src="/images/Telegraf_B8C5/chronograf.png" border="0"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Notice that Telegraf has also added agent-host label with the IP value of the SNMP Agent&lt;/li&gt;    &lt;li&gt;The values need to divide by 10 to get the temperature in Celsius; Router OS is doing this so it can give out the fraction part using INTEGER data type through SNMP&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;In the Docker Compose; we exposed/mapped the InfluxDB HTTP port; we can run the InfluxDB queries from the host directly using CURL etc to debug/see what�s going on&lt;/p&gt;  &lt;p&gt;&lt;a href="/images/Telegraf_B8C5/influxdb_3.png"&gt;&lt;img width="498" height="480" title="influxdb" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="influxdb" src="/images/Telegraf_B8C5/influxdb.png" border="0"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;p&gt;Interestingly; Telegraf has a &lt;a href="https://github.com/influxdata/telegraf/tree/master/plugins/outputs/prometheus_client" target="_blank"&gt;Prometheus Client Service Output&lt;/a&gt; plugin with which we can use it with Prometheus. Prometheus is based on pull model; this plugin starts the HTTP listener where it publishes the retrieved metrices and from where Prometheus can pull. To set it up; lets configure the plugin in telegraf.conf�s output section, bring in Prometheus configuring it to poll from the plugin endpoint. When we will bring things online; Telegraf will start pushing the metrics to InfluxDB as well as make them available at the Prometheus Client endpoint from where Prometheus will start polling accordingly&lt;/p&gt;  &lt;p&gt;&lt;img width="1301" height="588" title="prometheus" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="prometheus" src="/images/Telegraf_B8C5/prometheus.png" border="0"&gt;&lt;/p&gt;  &lt;p&gt;Once the data is in Prometheus; we can even bring Grafana and can start making Grafana dashboards.&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Refer to &lt;a title="Prometheus" href="/prometheus" target="_blank"&gt;Prometheus&lt;/a&gt; blog post for more information&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;a href="/images/Telegraf_B8C5/prometheus-graph_3.png"&gt;&lt;img width="640" height="433" title="prometheus-graph" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="prometheus-graph" src="/images/Telegraf_B8C5/prometheus-graph.png" border="0"&gt;&lt;/a&gt;&lt;a href="/images/Telegraf_B8C5/grafana_3.png"&gt;&lt;img width="527" height="480" title="grafana" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana" src="/images/Telegraf_B8C5/grafana.png" border="0"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;p&gt;I like Telegraf with its SNMP plugin more than Prometheus� SNMP-Exporter and having Telegraf in the environment opens up and enables more possibilities. Grafana also supports InfluxDB and we can have the dashboards using both Prometheus and InfluxDB as Time series data sources. While graphing Bandwidth COUNTERs from SNMP in Grafana, you will need to use Prometheus� rate() function and InfluxDB�s derivative() function.&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;The rate() function calculates the per second average rate of increase of the time series in the range vector given as parameter; below it will give the average rate of increase in 5min. The counter resets are automatically get adjusted&lt;/li&gt;    &lt;li&gt;The derivative() function returns the rate of change between the subsequent field values and converts the results into the rate of change per unit; given as second parameter; below given we need /sec rate of the bandwidth we are giving 1s as the second parameter&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;The InfluxDB derivative() function seems more close to how classic RRD/MRTG graph the bandwidth counters&lt;/p&gt;  &lt;p&gt;&lt;a href="/images/Telegraf_B8C5/grafana-prometheus_3.png"&gt;&lt;img width="492" height="480" title="grafana-prometheus" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-prometheus" src="/images/Telegraf_B8C5/grafana-prometheus.png" border="0"&gt;&lt;/a&gt;&lt;a href="/images/Telegraf_B8C5/grafana-influxdb_3.png"&gt;&lt;img width="457" height="480" title="grafana-influxdb" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-influxdb" src="/images/Telegraf_B8C5/grafana-influxdb.png" border="0"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Note that Grafana offers a rich InfluxDB query editor; and if you want to can switch to text mode where you can write InfluxDB queries directly&lt;/li&gt;    &lt;li&gt;Note that writing InfluxDB queries can become cumbersome for system administrators; they will like Prometheus more while developers might find InfluxDB more powerful and feel comfortable due to its RDBMS like queries&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;If you want to; you can remove Chronograf and even InfluxDB, and can use Telegraf directly with Prometheus / Grafana setup, or you can use Grafana with InfluxDB and not use Chronograf for dashboards. Its totally your preference!&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Docker compose, container configuration and all other required files are available in Tick folder @ &lt;a title="https://github.com/khurram-aziz/HelloDocker" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img width="611" height="480" align="right" style="margin:10px 10px 0px 0px;float:right;display:inline;" src="/images/0af903c6ed63_A51F/Tick-Stack-Complete.png"&gt;  &lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/influxdb</id>
		<title>InfluxDB</title>
		<link href="/posts/influxdb" />
		<updated>2018-06-22T00:00:00Z</updated>
		<content>&lt;table cellspacing="10" cellpadding="10"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;TICK Series&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;             &lt;li&gt;This Post&lt;/li&gt;  &lt;li&gt;&lt;a href="/telegraf"&gt;Telegraf&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;td&gt;&lt;p&gt;&lt;b&gt;Time Series Databases&lt;/b&gt;&lt;/p&gt;&lt;ul&gt;     &lt;li&gt;&lt;a href="/prometheus" target="_blank"&gt;Prometheus&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;This Post&lt;/li&gt;  &lt;li&gt;&lt;a href="/elk" target="_blank"&gt;ELK&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;p&gt;&lt;a href="/images//0af903c6ed63_A51F/Tick-Stack-Complete.png"&gt;&lt;img width="611" height="480" title="Tick-Stack-Complete" align="right" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;float:right;display:inline;background-image:none;" alt="Tick-Stack-Complete" src="/images//0af903c6ed63_A51F/Tick-Stack-Complete_thumb.png" border="0"&gt;&lt;/a&gt;           InfluxDB is another open source time series database and is written in Go language. It has no external dependency and its data model consists of several key-value pairs called the fieldset. Each point has a timestamp a value and fieldset. Timestamp and fieldset form a tagset and each point gets indexed by its timestamp and fieldset. Collection of tagsets form a series and multiple series can be grouped together by a string identifier to form a measurement. The measurement has a retention policy that defines how data gets downsampled and deleted. InfluxDB has SQL-like query engine having builtin time-centric functions for querying data. �Continuous Queries� can run periodically (and automatically by the database engine) storing results in a target measurement. It can listen on HTTP, TCP, and UDP where it accepts a data using its line protocol that is very similar to Graphite.&lt;/p&gt;  &lt;p&gt;The InfluxDB also has a commercial option; which is a distributed storage cluster giving horizontal scalability with storage and queries being handled by many nodes at once. The data gets sharded into the cluster nodes and it gets consistent eventually. We can query the cluster that runs sort of like MapReduce job. &lt;/p&gt;  &lt;h1&gt;InfluxDB vs Prometheus&lt;/h1&gt;  &lt;p&gt;&lt;a href="/images//0af903c6ed63_A51F/Tick-Stack-Complete.png"&gt;&lt;/a&gt;InfluxDB has nanosecond resolution while Prometheus has millisecond, InfluxDB supports int64, float64, bool, and string data types using different compression schemes for each one while Prometheus only supports float64. Prometheus approach of High Availability is to run multiple Prometheus nodes in parallel with no eventual consistency; and its Alertmanager than handles the deduplication and grouping. InfluxDB writes are durable while Prometheus buffers the writes in the memory and flushes them periodically (by default each 5min). InfluxDB has &lt;a href="https://docs.influxdata.com/influxdb/v1.5/query_language/continuous_queries/" target="_blank"&gt;Continuous Queries&lt;/a&gt; and Prometheus has &lt;a href="https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/" target="_blank"&gt;Recording Rules&lt;/a&gt;. Both does data compression and offer extensive integrations; including with each other. Both offers hooks and APIs to extend them further.&lt;/p&gt;  &lt;p&gt;Prometheus is simple, more performant and suits more for metrics. Its simpler storage model, simpler query language, alerting and notification functionality suits more to system administrators. That said; Prometheus being a PULL model; the server needs access to the nodes to retrieve the metrices and it might not suite in scenarios like IoT where devices are behind Wifi Gateway; or polling metrices from office machines that are behind NAT. Prometheus doesn't allow recording past data; in case you are extracting some time series data from some hardware logger; but InfluxDB let you record such data. In such situations where Prometheus is not full filling your requirements or where you need RDBMS like functionality against the time series data; we can use InfluxDB&lt;/p&gt;  &lt;h1&gt;IoT Example&lt;/h1&gt;  &lt;p&gt;For this post I am using Internet of Thing (IoT) scenario. Lets revisit an olt &lt;a href="/johnny-five" target="_blank"&gt;IoT post&lt;/a&gt; in which we used ESP8266 to measure room temperature using the sensor and send the readings to �ThingSpeak� service to view the temperature readings over time in the chart. In this post; we will try to remove ThingSpeak dependency using the �TICK stack�, the InfluxDB is an integral component of. TICK is an open source Time Series Platform for handling metrics and events and it consists of &lt;a href="https://github.com/influxdata/telegraf" target="_blank"&gt;Telegraf&lt;/a&gt;, &lt;a href="https://github.com/influxdata/influxdb" target="_blank"&gt;InfluxDB&lt;/a&gt;, &lt;a href="https://github.com/influxdata/chronograf" target="_blank"&gt;Chronograf&lt;/a&gt;, and &lt;a href="https://github.com/influxdata/kapacitor" target="_blank"&gt;Kapacitor&lt;/a&gt; open source projects all written in Go language. Chronograph is an administrative user interface and visualization engine. We need it to run InfluxQL; the SQL like queries against the data in InfluxDB. It also offers templates and libraries to build dashboards with real-time visualizations of time series data like Grafana&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;TICK picture taken from &lt;a href="https://www.influxdata.com/time-series-platform"&gt;https://www.influxdata.com/time-series-platform&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;blockquote&gt;   &lt;p&gt;Some might argue that we can use Prometheus with Pushgateway; the IoT devices can push their metrices to the Pushgateway that get hosted at the known location; its a valid argument and yes we can use it instead; as it acts as a buffer and Prometheus polls the metrices off the gateway periodically; the data sampling will not get reflected in the Time series database. Prometheus client libraries when used with Pushgateway usually sends all the metrices; even if one value is changed and needs the push; this increases network traffic and load on the sender; not something good for IoT scenario. Lastly Pushgateway remembers all the metrices even if they are no longer being pushed from the client; so for instance an IoT device is sending its IP address or host name in the metric; it will get remembered and next time if it gets different IP (usually the case in Wifi/behind NAT) it will get remembered as separate metric in Pushgateway and given Prometheus is polling off Pushgateway it will keep recording no longer required metrices as well. The Pushgateway has an option to group the metrices and we can delete the whole group using Pushgateway HTTP/Web API; but its not very ideal&lt;/p&gt; &lt;/blockquote&gt;  &lt;p&gt;The most convenient way to spin up the TICK stack is by using Docker. Lets create a simple docker-compose.yml file having InfluxDB and Chronograf and spin it up. We can then access Chronograf and can explore InfluxDB where it has created _internal database logging its own metrics&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;All required files are available in Tick folder @ &lt;a title="https://github.com/khurram-aziz/HelloDocker" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;&lt;img width="370" height="480" title="docker-compose" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="docker-compose" src="/images//0af903c6ed63_A51F/docker-compose.png" border="0"&gt;&lt;a href="/images//0af903c6ed63_A51F/chronograf.gif"&gt;&lt;img width="625" height="480" title="chronograf" style="margin:10px 10px 0px 0px;display:inline;" alt="chronograf" src="/images//0af903c6ed63_A51F/chronograf_thumb.gif"&gt;&lt;/a&gt;&lt;/p&gt;  &lt;p&gt;Lets extract the default �configuration� file of InfluxDB from its docker image first to build upon our configuration&lt;/p&gt;  &lt;p&gt;&lt;img width="637" height="416" title="influxdb-conf" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="influxdb-conf" src="/images//0af903c6ed63_A51F/influxdb-conf.png" border="0"&gt;&lt;/p&gt;  &lt;p&gt;Next enable InfluxDB UDP interface by adding udp section in the influxdb.conf; also map this UDP port to Docker Host and allow incoming UDP traffic to the mapped port in host�s firewall so that our IoT device can send its metrics on the known IP/Port of our Docker host&lt;/p&gt;  &lt;p&gt;&lt;img width="1146" height="616" title="udp" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="udp" src="/images//0af903c6ed63_A51F/udp.png" border="0"&gt;&lt;/p&gt;  &lt;p&gt;Now for the IoT firmware in Arduino; we just need to use the WiFiUDP instance from WiFiUDP.h and send the metric data using the InfluxDB line protocol; if we want to name our measurement temperature, and want to send device name, its local ip and raw sensor value along with the calculated temperature; we need to send following string in the udp packet&lt;/p&gt;  &lt;p&gt;temperature,device=DEVICENAME,localIP=ITS-IP,sensorValue=S value=T&lt;/p&gt;  &lt;p&gt;where S is sensor value and T is calculated temperature; our loop() function in Arduino will look something like this:&lt;/p&gt;  &lt;p&gt;&lt;img width="1447" height="557" title="arduino" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="arduino" src="/images//0af903c6ed63_A51F/arduino.png" border="0"&gt;&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;For more details see &lt;a title="https://www.influxdata.com/blog/how-to-send-sensor-data-to-influxdb-from-an-arduino-uno/" href="https://www.influxdata.com/blog/how-to-send-sensor-data-to-influxdb-from-an-arduino-uno/" target="_blank"&gt;https://www.influxdata.com/blog/how-to-send-sensor-data-to-influxdb-from-an-arduino-uno/&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;Bringing InfluxDB and Chronograf online; the data from IoT device will start logging and we can view the temperature graph in the Chronograf and export the raw data as CSV easily&lt;/p&gt;  &lt;p&gt;&lt;img width="1005" height="811" title="influxdb-temp-graph" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="influxdb-temp-graph" src="/images//0af903c6ed63_A51F/influxdb-temp-graph.png" border="0"&gt;&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;The Arduino code; docker compose and container configuration files are available in Tick folder @ &lt;a title="https://github.com/khurram-aziz/HelloDocker" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;  &lt;p&gt;In real world; Raspberry/Orange PIs can be used in remote cabinets with off the shelf / industrial strength temperature/humaditiy sensors. The boards can be connected to the switches directly due to their ethernet ports. These devices run full fledge Linux; you can access them remotely; run administrative scripts / commands; these boards also have USB ports where you can connect deployed devices administrative / serial ports. No need to send someone with laptops and physically connect in emergencies&lt;/p&gt;  &lt;p&gt;&lt;img width="640" height="480" title="raspberry-pi" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="raspberry-pi" src="/images//0af903c6ed63_A51F/raspberry-pi.jpg" border="0"&gt;&lt;img width="640" height="360" title="orange-pi" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="orange-pi" src="/images//0af903c6ed63_A51F/orange-pi.jpg" border="0"&gt;&lt;/p&gt;  &lt;ul&gt;   &lt;li&gt;Pictures taken from internet as reference&lt;/li&gt; &lt;/ul&gt;
</content>
		<summary>&lt;p&gt;&lt;b&gt;TICK Series&lt;/b&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/swarm-and-prometheus-ii</id>
		<title>Swarm and Prometheus II</title>
		<link href="/posts/swarm-and-prometheus-ii" />
		<updated>2018-04-23T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;b&gt;Prometheus Series&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;             &lt;li&gt;&lt;a target="_blank" href="/prometheus"&gt;Prometheus&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus"&gt;Swarm and Prometheus&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;This Post&lt;/li&gt;  &lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Time Series Databases&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;    &lt;li&gt;&lt;a target="_blank" href="/prometheus"&gt;Prometheus&lt;/a&gt;&lt;/li&gt;    &lt;li&gt;&lt;a target="_blank" href="/influxdb"&gt;InfluxDB&lt;/a&gt;&lt;/li&gt;  &lt;/ul&gt;
&lt;p&gt;&lt;b&gt;Docker Swarm Series&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;             &lt;li&gt;&lt;a target="_blank" href="/docker-swarm"&gt;Docker Swarm&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/docker-registry"&gt;Docker Registry&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/jenkins"&gt;Jenkins&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus"&gt;Swarm and Prometheus&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;This Post&lt;/li&gt;  &lt;/ul&gt;
&lt;p&gt;In the last post; we had our application running and being monitored in the Swarm cluster; but we have few issues and in this post we will try to solve them.&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Swarm-and-Prometheus-II_C680/volumes.png"&gt;&lt;img title="volumes" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;float:right;display:inline;background-image:none;" alt="volumes" src="/images/Swarm-and-Prometheus-II_C680/volumes_thumb.png" align="right" width="499" border="0" height="480"&gt;&lt;/a&gt;&lt;a href="/images/Swarm-and-Prometheus-II_C680/swarm-inbalanced.png"&gt;&lt;img title="swarm-inbalanced" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;float:right;display:inline;background-image:none;" alt="swarm-inbalanced" src="/images/Swarm-and-Prometheus-II_C680/swarm-inbalanced_thumb.png" align="right" width="294" border="0" height="480"&gt;&lt;/a&gt;The first issue is that our containers are not deployed in balanced way; most of our containers are made to run on the Manager Node because these containers need the “configuration files” that we are providing to them using the “volumes” binding. We can remove the placement constraint from our stack deploy compose file; but these containers will not work as the files were not be found on the “worker nodes”. This can be fixed by either using absolute path as source of these configuration files and copying all these files to that particular path on each node (manually or using some script/Git trigger) or we can place these files at some network location that is mounted to the specified location as source on each node. The second way is that we make our own docker images that already has these configuration files; we will have to write Dockerfiles; build them and push them to the registry; similar to our application container; so each Swarm node can get the image when/where required&lt;/p&gt;      &lt;blockquote&gt;     &lt;p&gt;Docker 17.06 onwards we have something called “Docker Configs”; using which we can these configuration files “outside” the container image; we create these configs using the Docker CLI or Compose files; and these config files gets uploaded to Swarm Manager that encode it and keep it in its “store” and provides an HTTP API using which Swarm Nodes can “download / retrieve” these configs (or config files). The Compose file has the support of these configs and we can remove the volume entries and replace them with config entries.&lt;/p&gt;   &lt;/blockquote&gt;      &lt;p&gt;The second issue is; all the containers that we need to expose so we can access them; like Prometheus and Grafana needs to be at some known location so we can access them using the known node IP/DNS name. This restriction will continue to give Manager heavy Swarm cluster; something that we dont want; instead we want to keep Manager lightweight; and use Workers to do heavy lifting.&lt;/p&gt;      &lt;blockquote&gt;     &lt;p&gt;Given our containers are exposing HTTP services; we can setup a Reverse Proxy on a known location; usually Manager node&lt;/p&gt;   &lt;/blockquote&gt;      &lt;p&gt;Lets rewrite the Stack Deploy compose file replacing volume with the config entries and adding NGINX a popular reverse proxy. We will have to write another config file for Nginx that we can pass using the Docker Configs. The other benefit by using Docker Config we will get is that we no longer have to copy YML and config files to Swarm Manager; using docker-machine and pointing our environment to appropriate Swarm Manager node; we can deploy our stack from a remote machine (development box or some CI/CD system)&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Docker Configs can be string or binary data upto 500kb in size; for larger files its better that we create our custom image having the required configuration or content; say you have lot of files in Grafana dashboards; its better to create custom image and push to the registry and use its image path&lt;/li&gt;        &lt;li&gt;Docker Configs are not kept or transmitted in encrypted format; there is similar feature called Docker Secrets that should be used for keeping sensitive configuration for things like database connection strings or Grafana admin password in our project&lt;/li&gt;        &lt;li&gt;&lt;br&gt;&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;References&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;&lt;a target="_blank" href="https://docs.docker.com/engine/swarm/configs"&gt;https://docs.docker.com/engine/swarm/configs&lt;/a&gt;&lt;/li&gt;        &lt;li&gt;&lt;a target="_blank" href="https://docs.docker.com/engine/swarm/secrets"&gt;https://docs.docker.com/engine/swarm/secrets&lt;/a&gt;&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;Our docker-stack compose file will look like this; and we will end up with a balanced Swarm&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Swarm-and-Prometheus-II_C680/docker-stack.png"&gt;&lt;img title="docker-stack" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="docker-stack" src="/images/Swarm-and-Prometheus-II_C680/docker-stack_thumb.png" width="332" border="0" height="480"&gt;&lt;/a&gt;&lt;a href="/images/Swarm-and-Prometheus-II_C680/swarm-balanced.png"&gt;&lt;img title="swarm-balanced" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="swarm-balanced" src="/images/Swarm-and-Prometheus-II_C680/swarm-balanced_thumb.png" width="346" border="0" height="480"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;All the code and required files are available in Prometheus folder at &lt;a target="_blank" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;For the Nginx reverse proxying we dont need anything special given Swarm does the DNS based service lookup; our service containers can be anywhere and Reverse Proxy will be able to discover and Proxy them straight away&lt;/p&gt;      &lt;p&gt;&lt;img title="nginx-conf" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="nginx-conf" src="/images/Swarm-and-Prometheus-II_C680/nginx-conf.png" width="332" border="0" height="298"&gt;&lt;/p&gt;      &lt;p&gt;The third issue is; that we are monitoring the nodes; but not the Docker Engine running on it; how many containers are running etc. The Docker Daemon exposes rich information and there exists many solutions online; I linked one in the last post. There is also &lt;a target="_blank" href="https://github.com/google/cadvisor"&gt;Google’s cAdvisor&lt;/a&gt; that’s very popular in Kubernetes community. cAdvisor exposes itself as web interface and exposes Prometheus metrics out of the box @ /metrics&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;&lt;a target="_blank" href="https://www.ctl.io/developers/blog/post/monitoring-docker-services-with-prometheus"&gt;https://www.ctl.io/developers/blog/post/monitoring-docker-services-with-prometheus&lt;/a&gt; is the officially linked resource&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;Given Prometheus is adopted and now officially a Cloud Native Computing Foundation (CNCF) project; Docker Daemon since v1.13 now exposes its metrics for Prometheus. However its wrapped under the experimental flag and sadly my Swarm setup doesnt allow setting required flags for the Docker Daemon. You can try it out with the standard Docker Daemon by setting the experimental flag and setting Metrics endpoint as metrics-addr&lt;/p&gt;      &lt;p&gt;&lt;img title="docker-daemon" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="docker-daemon" src="/images/Swarm-and-Prometheus-II_C680/docker-daemon.png" width="1024" border="0" height="688"&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;&lt;a target="_blank" href="https://docs.docker.com/config/thirdparty/prometheus"&gt;https://docs.docker.com/config/thirdparty/prometheus&lt;/a&gt; for more details&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;You can then give the address of the machine running the Docker Daemon and setup the Prometheus job accordingly. The &lt;a target="_blank" href="https://grafana.com/dashboards?search=docker%20engine"&gt;Grafana dashboard for Docker Daemon&lt;/a&gt; are also available that you can use and customize&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Swarm-and-Prometheus-II_C680/prometheus.png"&gt;&lt;img title="prometheus" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="prometheus" src="/images/Swarm-and-Prometheus-II_C680/prometheus_thumb.png" width="561" border="0" height="480"&gt;&lt;/a&gt;&lt;a href="/images/Swarm-and-Prometheus-II_C680/grafana.png"&gt;&lt;img title="grafana" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana" src="/images/Swarm-and-Prometheus-II_C680/grafana_thumb.png" width="561" border="0" height="480"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;All the code and required files are available in Prometheus folder at &lt;a href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt;   &lt;/ul&gt;
</content>
		<summary>&lt;p&gt;&lt;b&gt;Prometheus Series&lt;/b&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/swarm-and-prometheus</id>
		<title>Swarm and Prometheus</title>
		<link href="/posts/swarm-and-prometheus" />
		<updated>2018-04-21T00:00:00Z</updated>
		<content>  &lt;table cellspacing="0" cellpadding="2"&gt;       &lt;tbody&gt;&lt;tr&gt;         &lt;td style="width:300px;vertical-align:top;"&gt;           &lt;p&gt;&lt;b&gt;Prometheus Series&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;             &lt;li&gt;&lt;a target="_blank" href="/prometheus"&gt;Prometheus&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;This Post&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus-ii"&gt;Swarm and Prometheus II&lt;/a&gt;&lt;/li&gt;  &lt;/ul&gt;  &lt;/td&gt;            &lt;td style="width:300px;vertical-align:top;"&gt;    &lt;p&gt;&lt;b&gt;Time Series Databases&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;    &lt;li&gt;&lt;a target="_blank" href="/prometheus"&gt;Prometheus&lt;/a&gt;&lt;/li&gt;    &lt;li&gt;&lt;a target="_blank" href="/influxdb"&gt;InfluxDB&lt;/a&gt;&lt;/li&gt;  &lt;/ul&gt;  &lt;/td&gt;    &lt;td style="width:300px;vertical-align:top;"&gt;           &lt;p&gt;&lt;b&gt;Docker Swarm Series&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;             &lt;li&gt;&lt;a target="_blank" href="/docker-swarm"&gt;Docker Swarm&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/docker-registry"&gt;Docker Registry&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/jenkins"&gt;Jenkins&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;This Post&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus-ii"&gt;Swarm and Prometheus II&lt;/a&gt;&lt;/li&gt;  &lt;/ul&gt;  &lt;/td&gt;  &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;      &lt;p&gt;To monitor the Docker Swarm and our application running in the cluster with Prometheus; we can craft a v3 Compose File. To continue from the previous post; for the Swarm; we will like to have our Node Exporter running on all nodes. Unfortunately the host networking and host process namespace is not supported when we do docker stack deploy onto the swarm. Node Exporter and the Docker image we are using supports the command line argument and we can tell it that required directories are someplace else and using Docker Volume support can map the host directories to the container and pass the mapped paths to Node Exporter accordingly&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;All the code and required files are available in Prometheus folder at &lt;a title="https://github.com/khurram-aziz/HelloDocker" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;&lt;img title="node-exporter" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="node-exporter" src="/images/Swarm-and-Prometheus_DBDE/node-exporter.png" width="1269" border="0" height="412"&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;We will have to expose the Node Exporter port&lt;/li&gt;        &lt;li&gt;Note that we are deploying the container globally so each participating node get its own container instance; we will end up having the Node metrics at http://swarm-node:9100 url of participating nodes&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;If we have three nodes and we want to have a friendly host names for them to use in prometheus.yml Job definition; we can use extra_hosts section in our compose file. Further; if we want to access Prometheus we need that its container gets scheduled at some known node; so we have http://some-known-name:port url to reach Prometheus. We can do this using the constraints section in the Prometheus service section of the compose file&lt;/p&gt;      &lt;p&gt;&lt;img title="prometheus" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="prometheus" src="/images/Swarm-and-Prometheus_DBDE/prometheus.png" width="1043" border="0" height="326"&gt;&lt;/p&gt;      &lt;p&gt;In the previous Prometheus post; we made a .NET 4 console app and ran it on the development box. For the Docker Swarm; we need something that can run in the cluster. We can code .NET Core Console and use &lt;a target="_blank" href="https://github.com/prometheus-net/prometheus-net"&gt;prometheus-net&lt;/a&gt; client library to expose the metrices. Our app will be deployed in the cluster so it will have multiple instances running; each container instance will get its own IP address and Prometheus label the metric with this information; but its a good idea that we also include our own labels to identify which metric is coming from where.&lt;/p&gt;      &lt;p&gt;&lt;img title="dotnet-core" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="dotnet-core" src="/images/Swarm-and-Prometheus_DBDE/dotnet-core.png" width="862" border="0" height="409"&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;&lt;img title="multi-stage-build" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;float:right;display:inline;background-image:none;" alt="multi-stage-build" src="/images/Swarm-and-Prometheus_DBDE/multi-stage-build.png" align="right" width="476" border="0" height="289"&gt;I am using machine, custom_gauge and custom_counter matrices from the previous post; however for the Swarm; have added name and os labels using the prometheus-net client library. These labels will be given Machine Name and its OS Version values&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;One important thing to deploy the custom Docker image on the Swarm is that it needs to be “available” or “accessible” on all Swarm Nodes; this can be easily done by having a Docker Registry. Docker now has &lt;a target="_blank" href="https://docs.docker.com/develop/develop-images/multistage-build/"&gt;Multi Stage Build&lt;/a&gt; option and we can avoid to give access to Docker Registry to development box where we are building the Docker image or having Jenkins (or similar build environment). The Swarm node can compile and build .NET application on its own using this Multi Stage Build thing; there exist seperate SDK and RUNTIME official docker images&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;For this to work; we need to upload all the required files to the Swarm node where we will initiate this multi-stage build thing. We can either SCP or use GIT (or something like it) for this&lt;/li&gt;        &lt;li&gt;You can still go the traditional way by building the image on the development box and pushing from there to your docker registry; or tar-zip it from the development box and import in each node individually; whichever way you choose; all the Swarm Nodes need to either have or know a way to access the required image to run&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;Now if we bring our stack up using docker stack deploy and deploy the Host Stats dashboard (see the previous Prometheus post); we can view and monitor the Node Exporter matrices of each Swarm Node&lt;/p&gt;      &lt;p&gt;&lt;img title="hostmon-nodes" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="hostmon-nodes" src="/images/Swarm-and-Prometheus_DBDE/hostmon-nodes.png" width="827" border="0" height="610"&gt;&lt;/p&gt;      &lt;p&gt;However our custom application’s dashboard is not how we want; our application is deployed and running on all nodes (that we can verify either from Docker CLI or using Visualizer; see first Docker Swarm post) but Prometheus is not retrieving matrices from all these nodes simultaneously instead it is getting it from one node first and then from second and so on. This is happening because we had “netcoreconsole:8000” as target entry in the job and Docker Swarm is doing DNS round robin load balancing. Note that the instance label that Prometheus is adding is same but our custom label name is different for three nodes&lt;/p&gt;      &lt;p&gt;&lt;img title="nosd-grafana" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="nosd-grafana" src="/images/Swarm-and-Prometheus_DBDE/nosd-grafana.png" width="820" border="0" height="333"&gt;&lt;/p&gt;      &lt;p&gt;Prometheus has &lt;a target="_blank" href="https://github.com/prometheus/prometheus/tree/master/discovery"&gt;Service Discovery&lt;/a&gt; options; we can use &lt;a target="_blank" href="https://docs.docker.com/docker-cloud/apps/service-links/"&gt;Docker Swarm service discovery&lt;/a&gt; support; the Swarm exposes a special tasks.service dns entry and it will have IP addresses of all the associated containers of the service. Instead of using static_configs entry we can use dns_sd_configs entry with this special DNS entry and Prometheus will discover all the nodes and start retrieving the matrices&lt;/p&gt;      &lt;p&gt;&lt;img title="dns-sd" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="dns-sd" src="/images/Swarm-and-Prometheus_DBDE/dns-sd.png" width="631" border="0" height="639"&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;For more Prometheus Service Discovery information see &lt;a href="https://prometheus.io/blog/2015/06/01/advanced-service-discovery"&gt;https://prometheus.io/blog/2015/06/01/advanced-service-discovery&lt;/a&gt;&lt;/li&gt;        &lt;li&gt;As an exercise; try to use DNS Service Discovery option with the Node Exporter Job as well&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;We can confirm that it has discovered all the containers from the Service Discovery interface; and graphing our metrics we should see values coming from all the participating nodes in parallel now&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Swarm-and-Prometheus_DBDE/sd-prom.png"&gt;&lt;img title="sd-prom" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="sd-prom" src="/images/Swarm-and-Prometheus_DBDE/sd-prom_thumb.png" width="423" border="0" height="480"&gt;&lt;/a&gt;&lt;a href="/images/Swarm-and-Prometheus_DBDE/sd-prom-graph.png"&gt;&lt;img title="sd-prom-graph" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="sd-prom-graph" src="/images/Swarm-and-Prometheus_DBDE/sd-prom-graph_thumb.png" width="549" border="0" height="480"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;This should automatically get reflected in Grafana as well&lt;/p&gt;      &lt;p&gt;&lt;img title="sd-grafana" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="sd-grafana" src="/images/Swarm-and-Prometheus_DBDE/sd-grafana.png" width="886" border="0" height="442"&gt;&lt;/p&gt;      &lt;p&gt;We now dont need to expose the Prometheus; we can deploy it on the Swarm anywhere; the Grafana will discover it in the Swarm using the Service DNS entry that Swarm makes available. We only need Grafana to be running at the known location in the Swarm&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;All the code and required files are available in Prometheus folder at &lt;a title="https://github.com/khurram-aziz/HelloDocker" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;I remained focus to our custom application and its monitoring when running in Swarm. If you want to monitor your Swarm; take a look at &lt;a title="https://stefanprodan.com/2017/docker-swarm-instrumentation-with-prometheus/" href="https://stefanprodan.com/2017/docker-swarm-instrumentation-with-prometheus"&gt;https://stefanprodan.com/2017/docker-swarm-instrumentation-with-prometheus&lt;/a&gt;; an excellent work done by Prodan; and he has made all that work available on &lt;a target="_blank" href="https://github.com/stefanprodan/swarmprom"&gt;Github&lt;/a&gt; as well&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;b&gt;Prometheus Series&lt;/b&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/prometheus</id>
		<title>Prometheus</title>
		<link href="/posts/prometheus" />
		<updated>2018-04-16T00:00:00Z</updated>
		<content>  &lt;table cellspacing="0" cellpadding="2"&gt;       &lt;tbody&gt;&lt;tr&gt;         &lt;td style="width:300px;vertical-align:top;"&gt;           &lt;p&gt;&lt;b&gt;Prometheus Series&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;  &lt;li&gt;This Post&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus"&gt;Swarm and Prometheus&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus-ii"&gt;Swarm and Prometheus II&lt;/a&gt;&lt;/li&gt;  &lt;/ul&gt;  &lt;/td&gt;  &lt;td style="width:300px;vertical-align:top;"&gt;  &lt;p&gt;&lt;b&gt;Time Series Databases&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;  &lt;li&gt;This Post&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/influxdb"&gt;InfluxDB&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/elk"&gt;ELK&lt;/a&gt;&lt;/li&gt;  &lt;/ul&gt;  &lt;/td&gt;  &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;  &lt;p&gt;A time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average. A time series database (TSDB) is a software system that is optimized for handling time series data, arrays of numbers indexed by time (a datetime or a datetime range). In some fields these time series are called profiles, curves, or traces. A time series of stock prices might be called a price curve. A time series of energy consumption might be called a load profile. A log of temperature values over time might be called a temperature trace. � Wikipedia&lt;/p&gt;      &lt;p&gt;&lt;img width="763" height="525" title="architecture" align="right" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;float:right;display:inline;background-image:none;" alt="architecture" border="0" src="/images/Prometheus_A469/architecture.png"&gt;&lt;/p&gt;      &lt;p&gt;&lt;a target="_blank" href="https://prometheus.io/"&gt;Prometheus&lt;/a&gt; is an open-source systems monitoring and alerting toolkit; most of it components are written in Go, making them easy to build and deploy as static binaries. Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes. It provides multi-dimensional data model with time series data identified by metric name and key/value pairs. It collects time series using HTTP pulls, HTTP targets can be discovered via service discovery or configuration files. It also features a query language and it comes with the web interface where you can explore the data using its query language and execute and plot it for casual / exploration purposes.&lt;/p&gt;      &lt;p&gt;Picture Credit: &lt;a href="https://prometheus.io/docs/introduction/overview"&gt;https://prometheus.io/docs/introduction/overview&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;There are client libraries that we can use to add instrumenting support and integration with &lt;a target="_blank" href="https://github.com/prometheus/prometheus"&gt;Prometheus Server&lt;/a&gt;. &lt;a href="https://prometheus.io/docs/instrumenting/clientlibs"&gt;https://prometheus.io/docs/instrumenting/clientlibs&lt;/a&gt; has the list of these libraries for different languages and platform. There is also a &lt;a target="_blank" href="https://github.com/prometheus/pushgateway"&gt;push gateway&lt;/a&gt; for scenarios where adding HTTP endpoint to the application/device/node is not possible. There are standalone �exporters� that can retrieve metrics from popular services like HAProxy, StatsD, Graphite etc; these exporters have HTTP endpoint where they make this retrieved data available from where Prometheus Server can poll. Prometheus Server also exposes its own metrics and monitor its own metrics. It stores the retrieved metrics into local files in a custom format but also optionally can integrate with remote storage systems.&lt;/p&gt;      &lt;p&gt;&lt;a target="_blank" href="https://github.com/prometheus/node_exporter"&gt;Node exporter&lt;/a&gt; is a Prometheus exporter for hardware and OS metrics exposed by *nix kernels, its written in Go; &lt;a target="_blank" href="https://github.com/martinlindhe/wmi_exporter"&gt;WMI exporter&lt;/a&gt; is the recommended exporter for Windows based machines that uses WMI for retrieving metrics. There are many exporters available. &lt;a target="_blank" href="https://prometheus.io/docs/instrumenting/exporters"&gt;https://prometheus.io/docs/instrumenting/exporters&lt;/a&gt; has the list&lt;/p&gt;      &lt;p&gt;&lt;a target="_blank" href="https://github.com/prometheus/alertmanager"&gt;Alertmanager&lt;/a&gt; is a seperate component that exposes its API over HTTP; Prometheus Server sends alerts to it. This component supports different alerting channels like Email, Slack etc and takes care of alerting concerns like grouping, silencing, dispatching and retrying etc.&lt;/p&gt;      &lt;p&gt;&lt;a target="_blank" href="https://grafana.com"&gt;Grafana&lt;/a&gt; is usually used on top of Prometheus; an open source tool that provides beautiful monitoring and metric analytics and dashboard features. Grafana has notion of data sources from where it collects data; and Prometheus is supported out of box.&lt;/p&gt;      &lt;p&gt;For this post; lets consider we have a .NET 4 app; running on an old Windows 2003 box; may be because its integrated with some hardware whose drivers are not available for latest version of Windows restricting ourselves to continue to use this legacy .NET framework version. We want to modernize our app by adding monitoring support and may be some sort of Web API so we can make new components elsewhere and integrate to it. In .NET 4 applications if we want to have a HTTP endpoint; we can either use WebServiceHost class from System.ServiceModel.Web library intended for WCF endpoints (but we can do text/html with it) or there exists an older version of Microsoft.AspNet.WebApi.SelfHost package on Nuget. For our scenario this nuget package suits more as it will enable us to expose Web Api in our application as well.&lt;/p&gt;      &lt;p&gt;&lt;img width="805" height="380" title="install-package" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="install-package" border="0" src="/images/Prometheus_A469/install-package.png"&gt;&lt;/p&gt;      &lt;p&gt;In our application Main/Startup code; we need to configure and run the HttpSelfHostServer; to have /status and /metrics pages along with /api http endpoint our configuration will look something like what�s shown in the picture; and then we can simply add a StatusController inheriting from the ApiController and write the required action methods&lt;/p&gt;      &lt;p&gt;&lt;img width="1037" height="823" title="controller" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="controller" border="0" src="/images/Prometheus_A469/controller.png"&gt;&lt;/p&gt;      &lt;p&gt;Dont forget to allow the required port in Windows firewall; as later Prometheus will be accessing this http endpoint from a remote machine (from Docker Host/VM) &lt;/p&gt;      &lt;p&gt;For the Prometheus; we need to expose our application�s metrics data in the text/plain content type. The format is available at &lt;a title="https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md" href="https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md"&gt;https://github.com/prometheus/docs/blob/master/content/docs/instrumenting/exposition_formats.md&lt;/a&gt;; I am just exposing three test metrics. Once we have it running; we can setup Prometheus components and Docker Containers are great way to try out new thing and there exists official images for Prometheus components. I am going to use the following Docker Compose file and using Docker for Windows (or Linux in some VM etc) can bring all these components online with just docker-compose up. For details read &lt;a title="docker-compose.html" href="/docker-compose"&gt;docker-compose.html&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;&lt;img width="967" height="960" title="docker-compose" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="docker-compose" border="0" src="/images/Prometheus_A469/docker-compose.png"&gt;&lt;/p&gt;      &lt;p&gt;In my setup; I have a Node exporter that will expose the Docker VM/Host metrics, an Alertmanager, Prometheus Server and Grafana. All the configuration files for Prometheus components are YML files similar to docker-compose and are included in the repository. These files along with the .NET 4 Console project is available at &lt;a title="https://github.com/khurram-aziz/HelloDocker" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt; under Prometheus folder&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Note that Node exporter container is run with host networking and host process namespace so that it can get the metrics of the host and bind its http endpoint on the host ip address.&lt;/li&gt;        &lt;li&gt;Prometheus is configured according to my Docker for Windows Networking setting, if its different for you; or you are using Linux host to run the Docker; change it accordingly. You will need to change target addresses of Node exporter and Custom jobs in prometheus.yml&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;&lt;img width="1362" height="753" title="docker-networking" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="docker-networking" border="0" src="/images/Prometheus_A469/docker-networking.png"&gt;&lt;/p&gt;      &lt;p&gt;The Alertmanager is being configured through its config file in a way that if some service that it is polling gets down for two minutes or the Node exporter reports that CPU is 10% or more for two minutes; it will send an alert on Slack. You need to specify the Web hook URL in the config file. Yon can change or add more rules as per your requirements. If you are not using Slack; and want the good old Email alerts; there are documentation and tutorials available online&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Important thing to note is; you can have Alertmanager setup to send alerts to some database as well; in case you want to log �incidents� for SLA or something&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;If we have all the things setup properly and running; we can check our custom app metrics url and explore Prometheus through its Web interface&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Prometheus_A469/prometheus.png"&gt;&lt;img width="619" height="480" title="prometheus" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="prometheus" border="0" src="/images/Prometheus_A469/prometheus_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Prometheus_A469/prometheus-query.png"&gt;&lt;img width="481" height="480" title="prometheus-query" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="prometheus-query" border="0" src="/images/Prometheus_A469/prometheus-query_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;The Grafana will also be up with �First Dashboard� showing our metrics in the time graph; feel free to play with the dashboard and see that Grafana has the rich Prometheus support, along with its query auto completion etc&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Prometheus_A469/grafana.png"&gt;&lt;img width="640" height="466" title="grafana" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana" border="0" src="/images/Prometheus_A469/grafana_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Prometheus_A469/grafana-prometheus.png"&gt;&lt;img width="556" height="480" title="grafana-prometheus" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-prometheus" border="0" src="/images/Prometheus_A469/grafana-prometheus_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;Grafana dashboards can be imported and exported as JSON files and there are many &lt;a target="_blank" href="https://grafana.com/dashboards"&gt;official and community built dashboards available&lt;/a&gt; that we can easily import and start using. For our Node exporter; we can easily find some nice already made dashboard there. We just need the dashboard ID (or its JSON) file and we can easily import it into our Grafana setup&lt;/p&gt;      &lt;p&gt;&lt;a href="/images/Prometheus_A469/grafana-dashboards.png"&gt;&lt;img width="640" height="443" title="grafana-dashboards" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-dashboards" border="0" src="/images/Prometheus_A469/grafana-dashboards_thumb.png"&gt;&lt;/a&gt;&lt;a href="/images/Prometheus_A469/grafana-718.png"&gt;&lt;img width="640" height="389" title="grafana-718" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-718" border="0" src="/images/Prometheus_A469/grafana-718_thumb.png"&gt;&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;For importing from Grafana.com; we just need the ID of the dashboard; and update/configure the data source&lt;/p&gt;      &lt;p&gt;&lt;img width="640" height="355" title="grafana-import-id" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-import-id" border="0" src="/images/Prometheus_A469/grafana-import-id.png"&gt;&lt;img width="640" height="337" title="grafana-import" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-import" border="0" src="/images/Prometheus_A469/grafana-import.png"&gt;&lt;/p&gt;      &lt;p&gt;With few clicks; we will have a beautiful looking dashboard showing the Node exporter metrics&lt;/p&gt;      &lt;p&gt;&lt;img width="1197" height="669" title="grafana-node-dashboard" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-node-dashboard" border="0" src="/images/Prometheus_A469/grafana-node-dashboard.png"&gt;&lt;/p&gt;      &lt;p&gt;We know that Containers are immutable; and our customizations will be lost; unless we are mounting some volume and keeping the data there; or after edit/tweak the dashboard; or creating a new dashboard for our custom metrics; from Dashboard Settings we can export the JSON file. This JSON file then can be imported into our Docker Container when it is built and these dashboards will get provisioned automatically. The Grafana Container in our setup is configured accordingly; you can check Grafana YML and dashboards folder. Simply create a dashboard as per your liking and demand and then export the JSON file and remove its �id� and place the JSON file into the dashboards folder. This &lt;a target="_blank" href="http://docs.grafana.org/administration/provisioning/"&gt;Grafana Provisioning&lt;/a&gt; feature was introduced in v5&lt;/p&gt;      &lt;p&gt;In the repository I have just included one dashboard for our custom metrics; as an exercise try importing some dashboard from Grafana.com first, export its JSON file, place it in dashboards folder and rebuild the container!&lt;/p&gt;      &lt;p&gt;&lt;img width="1387" height="743" title="grafana-json" style="margin:10px 10px 0px 0px;border:0px currentcolor;border-image:none;display:inline;background-image:none;" alt="grafana-json" border="0" src="/images/Prometheus_A469/grafana-json.png"&gt;&lt;/p&gt;      &lt;p&gt;Happy Monitoring!&lt;/p&gt;      &lt;p&gt;Repository: &lt;a title="https://github.com/khurram-aziz/HelloDocker" href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;b&gt;Prometheus Series&lt;/b&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/dotnetcore-postgresql</id>
		<title>Dotnet Core - PostgreSQL</title>
		<link href="/posts/dotnetcore-postgresql" />
		<updated>2017-03-31T00:00:00Z</updated>
		<content>  &lt;p&gt;&lt;img width="221" height="228" title="postgresql" align="right" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;float:right;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="postgresql" src="/images/Postreg_A12D/postgresql.png" border="0"&gt;      &lt;b&gt;Dotnet Core Series&lt;/b&gt;&lt;/p&gt;  &lt;ul&gt;      &lt;li&gt;&lt;a href="/dotnet-core" target="_blank"&gt;Dotnet Core&lt;/a&gt;&lt;/li&gt;      &lt;li&gt;&lt;a href="/redis-clients-asp-net-core" target="_blank"&gt;Redis Clients -- ASP.NET Core&lt;/a&gt;&lt;/li&gt;      &lt;li&gt;&lt;a href="/docker-registry" target="_blank"&gt;Docker Registry&lt;/a&gt;&lt;/li&gt;      &lt;li&gt;&lt;a href="/jenkins" target="_blank"&gt;Jenkins&lt;/a&gt;&lt;/li&gt;      &lt;li&gt;This Post&lt;/li&gt;  &lt;/ul&gt;      &lt;p&gt;Given, Dotnet Core can run on Linux; and in the series we have been exploring different aspects of having a Microservice based Containerized Dotnet Core application running on a Linux in Docker Containers. SQL Server has been the defacto database for .NET application, there even exists &lt;a href="https://www.microsoft.com/en-us/sql-server/sql-server-vnext-including-linux" target="_blank"&gt;SQL Server for Linux&lt;/a&gt; (in a Public Preview form at the time of this post) and there is even an official image of &lt;a href="https://hub.docker.com/r/microsoft/mssql-server-linux" target="_blank"&gt;SQL Server for Linux for Docker Engine&lt;/a&gt; that we can use; and connect our existing beloved SQL Tools to connect to it; but it needs 3.25GB memory and its an evaluation version.&lt;/p&gt;      &lt;p&gt;&lt;img width="1574" height="770" title="db-sqlserver-docker" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="db-sqlserver-docker" src="/images/Postreg_A12D/db-sqlserver-docker.jpg" border="0"&gt;&lt;/p&gt;      &lt;p&gt;&lt;img width="469" height="480" title="db-pg-pgadmin" align="right" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;float:right;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="db-pg-pgadmin" src="/images/Postreg_A12D/db-pg-pgadmin.png" border="0"&gt;PostgreSQL is ACID compliant transactional object-relational database available for free on Windows, Linux and Mac. Its not as popular as mySQL; but it does provide serveral indexing functions, asynchronous commit, optimizer, synchronous and asynchronous replication that makes it technically more solid choice. Given its available for Windows; we can install it on the Windows development machines along with pgAdmin; the SQL Management Studio like client tool.&lt;/p&gt;    &lt;h1&gt;Entity Framework Core&lt;/h1&gt;    &lt;p&gt;&lt;a href="https://docs.microsoft.com/en-us/ef/core/" target="_blank"&gt;Entity Framework Core&lt;/a&gt; is a cross platform data access technology for Dotnet Core. Its not EF 7 or EF6.x compatible; its developed from scratch and supports many database engines through &lt;a href="https://docs.microsoft.com/en-us/ef/core/providers/index" target="_blank"&gt;Database Providers&lt;/a&gt;. &lt;a href="http://www.npgsql.org/" target="_blank"&gt;Npgsql&lt;/a&gt; is an excellent .NET data provider for PostreSQL (&lt;a href="https://github.com/npgsql" target="_blank"&gt;Their GitHub Repositories&lt;/a&gt;) and it supports &lt;a href="http://www.npgsql.org/efcore/index.html" target="_blank"&gt;EF Core&lt;/a&gt;. All you need to do is install the &lt;a href="https://www.nuget.org/packages/Npgsql.EntityFrameworkCore.PostgreSQL/" target="_blank"&gt;Npgsql.EntityFrameworkCore.PostgreSQL&lt;/a&gt; NUGET package using the dotnet core CLI (dotnet add package Npgsql.EntityFrameworkCore.PostgreSQL) It will bring along the EF Core and Npgsql libraries into your project.&lt;/p&gt;      &lt;p&gt;We can now write our Entity classes and a DbContext class. For Npgsql, in OnConfiguring override, we will use UseNpgsql instead of UseSqlServer with DbContextOptionsBuilder passing on required PostgreSQL connection string. Here’s one entity class and context file I made for testing!&lt;/p&gt;   &lt;script src="https://gist.github.com/khurram-aziz/c64d07170b499a4794644ec1b9e2da20.js"&gt;&lt;/script&gt;    &lt;p&gt;We can use the Context class with all the LINQ goodness; similar to SQL Server; for instance here’s the controller class&lt;/p&gt;   &lt;script src="https://gist.github.com/khurram-aziz/1952e3da90ec690414872e8613a0c17f.js"&gt;&lt;/script&gt;    &lt;p&gt;And here’s displaying the products in the View&lt;/p&gt;   &lt;script src="https://gist.github.com/khurram-aziz/409d733938174a70e53cfdb818799611.js"&gt;&lt;/script&gt;    &lt;ul&gt;     &lt;li&gt;If the entities are in different namespace; either import the namespace in the web.config file in the Views folder; or add the namespace in particular View by adding the using at the top &lt;b&gt;@using ECommMvc.Models;&lt;/b&gt; &lt;/li&gt;   &lt;/ul&gt;    &lt;h1&gt;Entity Framework Core Command Line Tools&lt;/h1&gt;    &lt;p&gt;&lt;a href="https://docs.microsoft.com/en-us/ef/core/miscellaneous/cli/dotnet" target="_blank"&gt;EF Core .NET Command Line Tools&lt;/a&gt; extends Dotnet CLI; and add ef commands to the dotnet (CLI) We need to add Microsoft.EntityFrameworkCore.Tools.Dotnet and Microsoft.EntityFrameworkCore.Design NUGET packages using dotnet add package NUGET; dotnet restore them and then add PackageReference using Design and DotnetCliToolReference using the Tools.Dotnet package; and you should end up having the dotnet ef commands in the project&lt;/p&gt;      &lt;p&gt;&lt;img width="939" height="763" title="db-eftools" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="db-eftools" src="/images/Postreg_A12D/db-eftools.png" border="0"&gt;&lt;/p&gt;      &lt;p&gt;Using the ef commands; we can add the Migration (dotnet ef migration add Migration-Name), remove it; update the database (dotnet ef database update) and more. Once we have the Migrations in place; we can continue to evolve our Entities and Database accordingly.&lt;/p&gt;    &lt;h1&gt;Seeding&lt;/h1&gt;    &lt;p&gt;Using the Migrations; we can Seed our database as well; we can create a Migration naming Seed; and add the required seeding code in the migration’s CS file&lt;/p&gt;   &lt;script src="https://gist.github.com/khurram-aziz/4e4588eb535e27cebd897acf7bb3601f.js"&gt;&lt;/script&gt;    &lt;p&gt;When deploying into the Docker Container; we often need “side kick” container that “seeds” the cache or database (for details see &lt;a href="http://weblogs.com.pk/khurram/archive/2017/01/04/dockerizing-php-mysql-application-part-2.aspx" target="_blank"&gt;Dockerizing PHP + MySQL Application Part 2&lt;/a&gt;); as when container is started we get the clean slate. Given the Migrations code become part of the MVC project; and in .NET Core; there is a Program.cs the entry point where Kestrel / MVC is initialized; we can add more code there as well. We can use the Context that’s in place and Migrations and update the database (that will initialize and seed)&lt;/p&gt;   &lt;script src="https://gist.github.com/khurram-aziz/46c72fa948b1292da17f09cb8b5f2a52.js"&gt;&lt;/script&gt;  &lt;h1&gt;Docker&lt;/h1&gt;    &lt;p&gt;Now with database work in place and Docker building techniques shown in previous posts (&lt;a title="Redis Clients -- ASP.NET Core" href="/redis-clients-asp-net-core"&gt;Redis Clients -- ASP.NET Core&lt;/a&gt; and &lt;a title="Jenkins" href="/jenkins"&gt;Jenkins&lt;/a&gt;); we can have v2 Compose file or v3 Compose file (for Docker Swarm) and deploy our .NET Core MVC application that’s using Redis as Caching and PostgreSQL as Database into Docker Containers running on Linux node(s)&lt;/p&gt;    &lt;script src="//gist-it.appspot.com/github/khurram-aziz/HelloDotnetCore/blob/master/docker-compose.yml"&gt;    &lt;ul&gt;     &lt;li&gt;&lt;/script&gt;Project is available at &lt;a title="https://github.com/khurram-aziz/HelloDotnetCore" href="https://github.com/khurram-aziz/HelloDotnetCore" target="_blank"&gt;https://github.com/khurram-aziz/HelloDotnetCore&lt;/a&gt;
</content>
		<summary>&lt;p&gt;&lt;img width="221" height="228" title="postgresql" align="right" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;float:right;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="postgresql" src="/images/Postreg_A12D/postgresql.png" border="0"&gt;      &lt;b&gt;Dotnet Core Series&lt;/b&gt;&lt;/p&gt;</summary>
	</entry>
	<entry>
		<id>/posts/jenkins</id>
		<title>Jenkins</title>
		<link href="/posts/jenkins" />
		<updated>2017-03-23T00:00:00Z</updated>
		<content>&lt;p&gt;&lt;img title="jenkins-logo" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;float:right;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="jenkins-logo" src="/images/Jenkins_11AFB/jenkins-logo.png" border="0" height="312" align="right" width="226"&gt;    &lt;/p&gt;&lt;table cellspacing="0" cellpadding="0"&gt;        &lt;tbody&gt;&lt;tr&gt;            &lt;td style="width:300px;vertical-align:top;"&gt;                &lt;p&gt;&lt;b&gt;Docker Swarm Series&lt;/b&gt;&lt;/p&gt;                &lt;ul&gt;                    &lt;li&gt;&lt;a target="_blank" href="/docker-swarm"&gt;Docker Swarm&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;&lt;a target="_blank" href="/docker-registry"&gt;Docker Registry&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;This Post&lt;/li&gt;    &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus"&gt;Swarm and Prometheus&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus-ii"&gt;Swarm and Prometheus II&lt;/a&gt;&lt;/li&gt;                &lt;/ul&gt;          &lt;/td&gt;            &lt;td style="width:300px;vertical-align:top;"&gt;                &lt;p&gt;&lt;b&gt;Dotnet Core Series&lt;/b&gt;&lt;/p&gt;                &lt;ul&gt;                    &lt;li&gt;&lt;a target="_blank" href="/dotnet-core"&gt;Dotnet Core&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;&lt;a target="_blank" href="/redis-clients-asp-net-core"&gt;Redis Clients -- ASP.NET Core&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;&lt;a target="_blank" href="/docker-registry"&gt;Docker Registry&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;This Post&lt;/li&gt;                    &lt;li&gt;&lt;a target="_blank" href="/dotnetcore-postgresql"&gt;Dotnet Core -- PostgreSQL&lt;/a&gt;&lt;/li&gt;              &lt;/ul&gt;          &lt;/td&gt;      &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;      &lt;p&gt;&lt;a target="_blank" href="https://jenkins.io/"&gt;Jenkins&lt;/a&gt; is an open source “automation server” developed in Java. Its more&amp;nbsp;than a typical build system / tool, its additional features and plugins; can help to automate the non-human part of the software development process. Its a server based system that needs a servlet container such as Tomcat. There are number of plugins available for integration with different version control systems and databases, setting up automated tests&amp;nbsp;using the framework of your choice including MSTest and NUnit and doing lot more during the build than compiling the code. If Java and Servlet Containers are not your "thing"; running Jenkins in the Docker provides enough black box around these that&amp;nbsp;getting started with it cant be more simpler.&amp;nbsp;There is an official image on the Docker Hub and we just need to map its two exposed port; one for its web interface and the other that its additional agents uses to connect to the server. All&amp;nbsp;that needs to run its instance is&amp;nbsp;&lt;b&gt;docker run –p 8080:8080 –p 50000:50000 jenkins&lt;/b&gt;; we can optionally map the container’s /var/jenkins_home to some local folder as well. To learn more options that can be set using the environment variables like JVM options; visit &lt;a href="https://hub.docker.com/_/jenkins"&gt;https://hub.docker.com/_/jenkins&lt;/a&gt;&lt;/p&gt;      &lt;p&gt;We can use Jenkins to build Dotnet Core projects; all we need is to install the Dotnet Core SDK on the system running the Jenkins. For the Container; we can expand the official image and write a Dockerfile to install the required things; but first we need to check which Linux jenkins image is based on and for that; do this&lt;/p&gt;      &lt;p&gt;&lt;img title="jenkins-image-linux" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="jenkins-image-linux" src="/images/Jenkins_11AFB/jenkins-image-linux.png" border="0" height="263" width="720"&gt;&lt;/p&gt;      &lt;p&gt;Knowing that its Debian 8 and its running the things under jenkins user; lets make a Dockerfile for our custom Jenkins docker image&lt;/p&gt;   &lt;script src="https://gist.github.com/khurram-aziz/e12c22199b097f06e62cf5ffdcf29e10.js"&gt;&lt;/script&gt;    &lt;ul&gt;     &lt;li&gt;Note that we used information from &lt;a target="_blank" href="https://www.microsoft.com/net/core#linuxdebian"&gt;https://www.microsoft.com/net/core#linuxdebian&lt;/a&gt; on how to install the latest Dotnet Core SDK on Debian &lt;/li&gt;        &lt;li&gt;Note that we added git plugin as per guideline at &lt;a title="https://github.com/jenkinsci/docker" target="_blank" href="https://github.com/jenkinsci/docker"&gt;https://github.com/jenkinsci/docker&lt;/a&gt; where this official image is maintained; they have provided the install-plugins.sh script; and we can install the plugins while making the image; and we will not have to reinstall these plugins when running the Jenkins &lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;If we build this Dockerfile and tag the image as dotjenkins; we can run it using docker run –rm –p 8080:8080 –p 50000:50000 dotjenkins; running on a console is required so we can get the initial admin password from the info that gets emitted; its required on the first run setup wizard when we will open the http://localhost:8080&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Visit &lt;a target="_blank" href="/dockerfile"&gt;Dockerfile&lt;/a&gt;; if you need the heads up on&amp;nbsp;how to&amp;nbsp;build Docker image from&amp;nbsp;such file&lt;/li&gt;    &lt;li&gt;During the setup; you can choose Custom Plugins; and then select none; and we will have the minimalist Jenkins ready to build the Dotnet Core project from Git Repository &lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;You can setup a test project; giving &lt;a href="https://github.com/khurram-aziz/HelloDocker"&gt;https://github.com/khurram-aziz/HelloDocker&lt;/a&gt; as Git path that has a Dotnet Core MVC application&lt;/p&gt;      &lt;p&gt;&lt;img title="jenkins-git" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="jenkins-git" src="/images/Jenkins_11AFB/jenkins-git.png" border="0" height="440" width="672"&gt;&lt;/p&gt;      &lt;p&gt;&amp;nbsp;&lt;/p&gt;      &lt;p&gt;&amp;nbsp;&lt;/p&gt;      &lt;p&gt;&lt;img title="jenkins-build-shell" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;float:right;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="jenkins-build-shell" src="/images/Jenkins_11AFB/jenkins-build-shell.png" border="0" height="258" align="right" width="329"&gt;&lt;img title="jenkins-build-shell-step" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;float:right;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="jenkins-build-shell-step" src="/images/Jenkins_11AFB/jenkins-build-shell-step.png" border="0" height="219" align="right" width="464"&gt;And then can use Execute shell Build step type and enter the familiar dotnet restore and dotnet build commands to build the Dotnet Core application with the Jenkins&lt;/p&gt;      &lt;p&gt;Once the test job is setup; we can build it; and it will download the code from Git and build it as per the steps we have defined. We can see the Console output from the web interface as well!&lt;/p&gt;      &lt;p&gt;&lt;img title="jenkins-build-consoleoutput" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="jenkins-build-consoleoutput" src="/images/Jenkins_11AFB/jenkins-build-consoleoutput.png" border="0" height="732" width="984"&gt;&lt;/p&gt;      &lt;p&gt;&amp;nbsp;&lt;/p&gt;      &lt;p&gt;If you are following the Dotnet Core Series; in the last post; &lt;a title="Docker Registry" target="_blank" href="/docker-registry"&gt;Docker Registry&lt;/a&gt;; we also needed to build the mvcapp Docker Container after publishing the Mvc application. In that post; the developer had to have the &lt;a target="_blank" href="https://docs.docker.com/docker-for-windows/"&gt;Docker for Windows&lt;/a&gt; installed, as to build the Docker image; we need the Docker Daemon; and we also needed the access of the Docker Registry so we can push the Mvc application as the Docker Container from where the Swarm Nodes will pick it when System Administrator will deploy the “Stack” on the &lt;a target="_blank" href="/docker-swarm"&gt;Docker Swarm&lt;/a&gt;. We can solve this using the Jenkins; that it can not only automate this manual work; but also we will neither need Docker for Windows at the developer machine nor will need to give access of Docker Registry to the developer.&lt;/p&gt;      &lt;p&gt;To build Docker Container Image; from Jenkins running in Docker Container; we first need to technically assess how it can be done.&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;We need to study the Dockerfile of Jenkins at its &lt;a target="_blank" href="https://github.com/jenkinsci/docker"&gt;GitHub Repo&lt;/a&gt;, as its creating jenkins user with uid 1000 and running the things under this user. &lt;/li&gt;        &lt;li&gt;We will need the Docker CLI tools in the container &lt;/li&gt;        &lt;li&gt;We will need the access of Docker Daemon in the container so that it can build the Docker Images using the Docker daemon &lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;Lets make a separate Dockerfile first to technically assess it without the Jenkins overhead.&lt;/p&gt;   &lt;script src="https://gist.github.com/khurram-aziz/975e57a6afebc1436e857594213660f6.js"&gt;&lt;/script&gt;    &lt;p&gt;If we build and tag the above Dockerfile as dockcli; we can run it as &lt;b&gt;docker run -v /var/run/docker.sock:/var/run/docker.sock -it dockcli&lt;/b&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Note that we exposed /var/run/docker.sock file as VOLUME in the Dockerfile; so that we can map it when running the container passing the docker.sock file of the Docker Daemon where its “launching” this way we dont need to run the Docker Daemon in the Container and we can “reuse” the Docker Daemon where our image will run; there exists “Docker in Docker” image using which we can run the Docker Daemon inside the Container; but we dont need it here &lt;/li&gt;        &lt;li&gt;We created a jenkins user similar to how its made and configured in the official Jenkins image &lt;/li&gt;        &lt;li&gt;We need to add jenkins into sudoers (and for that we also need to install sudo first) so we can access docker.sock using sudo; else it will not have enough permissions &lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;With these arrangements we can access the Docker Daemon from with in the Docker Container!&lt;/p&gt;      &lt;p&gt;&lt;img title="jenkins-sudo" style="border-top:0px;border-right:0px;background-image:none;border-bottom:0px;padding-top:0px;padding-left:0px;border-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;" alt="jenkins-sudo" src="/images/Jenkins_11AFB/jenkins-sudo.png" border="0" height="352" width="1032"&gt;&lt;/p&gt;      &lt;p&gt;Now lets add back this work to our dotjenkins Dockerfile; so we can create the Docker Image after the Dotnet Core build in the Jenkins. Here’s the final Dockerfile&lt;/p&gt;      &lt;script src="https://gist.github.com/khurram-aziz/6371de03771fb9b9b7478472193a5c07.js"&gt;&lt;/script&gt;    &lt;p&gt;Lets run our customized Jenkins image using &lt;b&gt;docker run --name jenk --rm -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dotjenkins&lt;/b&gt;&lt;/p&gt;      &lt;p&gt;We can now add Docker Image Creation step in our project build steps&lt;/p&gt;      &lt;p&gt;&lt;img title="jenkins-build-dockerstep" style="border-top:0px;border-right:0px;background-image:none;border-bottom:0px;padding-top:0px;padding-left:0px;border-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;" alt="jenkins-build-dockerstep" src="/images/Jenkins_11AFB/jenkins-build-dockerstep.png" border="0" height="223" width="508"&gt;&lt;/p&gt;      &lt;ul&gt;     &lt;li&gt;Note the usage of Jenkins variable as the tag for the image we are creating&lt;/li&gt;   &lt;/ul&gt;      &lt;p&gt;If we do a Jenkins build; we will see the Docker step output in the Console Output and shortly afterwords we will have the required image in the Docker Daemon where Jenkins Container is running&lt;/p&gt;      &lt;p&gt;&lt;img title="jenkins-build-dockeroutput" style="border-top:0px;border-right:0px;background-image:none;border-bottom:0px;padding-top:0px;padding-left:0px;border-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;" alt="jenkins-build-dockeroutput" src="/images/Jenkins_11AFB/jenkins-build-dockeroutput.png" border="0" height="304" width="1150"&gt;&lt;/p&gt;      &lt;p&gt;We can use &lt;a target="_blank" href="/docker-compose"&gt;docker-compose&lt;/a&gt; to define all the related switches needed to run the docker image so it becomes a simple docker-compose up command. On the similar lines; we can now add additional step to push the generated Docker image to the Docker Registry!&lt;/p&gt;&lt;p&gt;&lt;/p&gt;
</content>
		<summary>&lt;p&gt;&lt;img title="jenkins-logo" style="border-left-width:0px;border-right-width:0px;background-image:none;border-bottom-width:0px;float:right;padding-top:0px;padding-left:0px;margin:10px 10px 0px 0px;display:inline;padding-right:0px;border-top-width:0px;" alt="jenkins-logo" src="/images/Jenkins_11AFB/jenkins-logo.png" border="0" height="312" align="right" width="226"&gt;    &lt;/p&gt;&lt;table cellspacing="0" cellpadding="0"&gt;        &lt;tbody&gt;&lt;tr&gt;            &lt;td style="width:300px;vertical-align:top;"&gt;                &lt;p&gt;&lt;b&gt;Docker Swarm Series&lt;/b&gt;&lt;/p&gt;                &lt;ul&gt;                    &lt;li&gt;&lt;a target="_blank" href="/docker-swarm"&gt;Docker Swarm&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;&lt;a target="_blank" href="/docker-registry"&gt;Docker Registry&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;This Post&lt;/li&gt;    &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus"&gt;Swarm and Prometheus&lt;/a&gt;&lt;/li&gt;  &lt;li&gt;&lt;a target="_blank" href="/swarm-and-prometheus-ii"&gt;Swarm and Prometheus II&lt;/a&gt;&lt;/li&gt;                &lt;/ul&gt;          &lt;/td&gt;            &lt;td style="width:300px;vertical-align:top;"&gt;                &lt;p&gt;&lt;b&gt;Dotnet Core Series&lt;/b&gt;&lt;/p&gt;                &lt;ul&gt;                    &lt;li&gt;&lt;a target="_blank" href="/dotnet-core"&gt;Dotnet Core&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;&lt;a target="_blank" href="/redis-clients-asp-net-core"&gt;Redis Clients -- ASP.NET Core&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;&lt;a target="_blank" href="/docker-registry"&gt;Docker Registry&lt;/a&gt;&lt;/li&gt;                    &lt;li&gt;This Post&lt;/li&gt;                    &lt;li&gt;&lt;a target="_blank" href="/dotnetcore-postgresql"&gt;Dotnet Core -- PostgreSQL&lt;/a&gt;&lt;/li&gt;              &lt;/ul&gt;          &lt;/td&gt;      &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;      &lt;p&gt;&lt;/p&gt;</summary>
	</entry>
</feed>